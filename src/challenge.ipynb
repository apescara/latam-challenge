{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu soluciÃ³n y todas las suposiciones que estÃ¡s considerando. AquÃ­ puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pregunta 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se tomaron dos enfoques distintos. El primero consistiÃ³ en leer los datos con la librerÃ­a JSON y luego trabajar la informaciÃ³n con Pandas. Para el segundo enfoque se utilizÃ³ PySpark para la lectura y tratamiento de los datos.\n",
    "\n",
    "En cuanto al tiempo de ejecuciÃ³n con el primer mÃ©todo, se observÃ³ que los tiempos eran estables y repetibles, con una duraciÃ³n de alrededor de 11 segundos. En cambio, el segundo mÃ©todo dependÃ­a de forma sustancial del estado de la sesiÃ³n de Spark. En la primera ejecuciÃ³n consecutiva, los tiempos de ejecuciÃ³n rondaban los 15 segundos, muy distintos a los tiempos de las ejecuciones posteriores, que rondaban entre 5 y 6 segundos.\n",
    "\n",
    "Debido a esto, se tomÃ³ la decisiÃ³n de elegir el primer mÃ©todo para q1_time, Ãºnicamente por su estabilidad. Cabe considerar que, en el caso de tener un ambiente de ejecuciÃ³n mÃ¡s estable, el segundo mÃ©todo podrÃ­a ofrecer mejores resultados a largo plazo.\n",
    "\n",
    "En cuanto al uso de memoria, este fue un anÃ¡lisis mÃ¡s directo. Con el mÃ©todo 1, siempre se observÃ³ un uso de hasta 1300 MiB, muy por encima del mÃ©todo 2, que rondaba los 300 MiB de forma constante. Por ende, se seleccionÃ³ el mÃ©todo 2 para q1_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metodo 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ejecuto multiples veces y siempre se consigui un tiempo de ejecucion de alrededor de 11s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q1_time import  q1_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 11.0353 s\n",
      "File: /home/sandbox/files/latam-challenge/src/q1_time.py\n",
      "Function: q1_time at line 8\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     8                                           def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "     9                                           \n",
      "    10         1       1182.0   1182.0      0.0      def read_json(file_path):\n",
      "    11                                                   data = []\n",
      "    12                                                   with open(file_path, \"r\") as f:\n",
      "    13                                                       # En el archivo hay un json por linea, hay que itearr y extrar cada uno\n",
      "    14                                                       for line in f:\n",
      "    15                                                           data.append(json.loads(line))\n",
      "    16                                                       f.close()\n",
      "    17                                           \n",
      "    18                                                   return data\n",
      "    19                                           \n",
      "    20                                               # Leer json y pasar a DF\n",
      "    21         1 6526351664.0    7e+09     59.1      data = read_json(file_path=file_path)\n",
      "    22         1  772109664.0    8e+08      7.0      data = pd.DataFrame.from_records(data)\n",
      "    23                                           \n",
      "    24                                               # Arreglar datos a utilizar\n",
      "    25         1 3453171372.0    3e+09     31.3      data[\"date_part\"] = data.date.apply(lambda x: pendulum.parse(x).date())\n",
      "    26         1   64078844.0    6e+07      0.6      data[\"username\"] = data.user.apply(lambda x: x[\"username\"])\n",
      "    27                                           \n",
      "    28                                               # Seleccionar y agrupar data a usar\n",
      "    29         1        351.0    351.0      0.0      data_group = (\n",
      "    30         1    6260832.0    6e+06      0.1          data[[\"date_part\", \"username\", \"id\"]]\n",
      "    31         1     411679.0 411679.0      0.0          .groupby([\"date_part\", \"username\"])\n",
      "    32         1   80349934.0    8e+07      0.7          .count()\n",
      "    33         1    6810530.0    7e+06      0.1          .reset_index()\n",
      "    34         1    5809088.0    6e+06      0.1          .sort_values(by=\"id\", ascending=False)\n",
      "    35                                               )\n",
      "    36                                           \n",
      "    37                                               # Calcular resultados, se obtendra por un lado los dias mas activos y por el otro los usuarios mas activos por cada dÃ­a.\n",
      "    38                                               # Finalmente se realizara el cruce de los 2\n",
      "    39         1        140.0    140.0      0.0      most_active_dates = (\n",
      "    40         1     371425.0 371425.0      0.0          data_group.groupby(\"date_part\")[\"id\"]\n",
      "    41         1    3165197.0    3e+06      0.0          .sum()\n",
      "    42         1     198000.0 198000.0      0.0          .sort_values(ascending=False)\n",
      "    43         1      80521.0  80521.0      0.0          .head(10)\n",
      "    44         1       1393.0   1393.0      0.0          .index\n",
      "    45                                               )\n",
      "    46         1        171.0    171.0      0.0      most_active_user_by_date = (\n",
      "    47         1     191358.0 191358.0      0.0          data_group.groupby([\"date_part\", \"username\"])[\"id\"]\n",
      "    48         1   91447367.0    9e+07      0.8          .sum()\n",
      "    49         1     799014.0 799014.0      0.0          .sort_values(ascending=False)\n",
      "    50         1     425886.0 425886.0      0.0          .groupby(level=0)\n",
      "    51         1   20543119.0    2e+07      0.2          .idxmax()\n",
      "    52         1     386322.0 386322.0      0.0          .apply(lambda x: x[1])\n",
      "    53                                               )\n",
      "    54         2     513800.0 256900.0      0.0      most_active_dates_with_most_active_users = most_active_user_by_date.loc[\n",
      "    55         1        120.0    120.0      0.0          most_active_dates\n",
      "    56                                               ]\n",
      "    57                                           \n",
      "    58                                               # Dar formato final\n",
      "    59         1        230.0    230.0      0.0      out = []\n",
      "    60        11    1475037.0 134094.3      0.0      for index, row in most_active_dates_with_most_active_users.reset_index().iterrows():\n",
      "    61        10     343882.0  34388.2      0.0          out.append((row.iloc[0], row.iloc[1]))\n",
      "    62                                           \n",
      "    63         1        170.0    170.0      0.0      return out"
     ]
    }
   ],
   "source": [
    "%lprun -f q1_time q1_time(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 10.5336 s\n",
      "File: /home/sandbox/files/latam-challenge/src/q1_time.py\n",
      "Function: q1_time at line 8\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     8                                           def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "     9                                           \n",
      "    10         1       1182.0   1182.0      0.0      def read_json(file_path):\n",
      "    11                                                   data = []\n",
      "    12                                                   with open(file_path, \"r\") as f:\n",
      "    13                                                       # En el archivo hay un json por linea, hay que itearr y extrar cada uno\n",
      "    14                                                       for line in f:\n",
      "    15                                                           data.append(json.loads(line))\n",
      "    16                                                       f.close()\n",
      "    17                                           \n",
      "    18                                                   return data\n",
      "    19                                           \n",
      "    20                                               # Leer json y pasar a DF\n",
      "    21         1 6480833095.0    6e+09     61.5      data = read_json(file_path=file_path)\n",
      "    22         1  792292537.0    8e+08      7.5      data = pd.DataFrame.from_records(data)\n",
      "    23                                           \n",
      "    24                                               # Arreglar datos a utilizar\n",
      "    25         1 3015026849.0    3e+09     28.6      data[\"date_part\"] = data.date.apply(lambda x: pendulum.parse(x).date())\n",
      "    26         1   73338453.0    7e+07      0.7      data[\"username\"] = data.user.apply(lambda x: x[\"username\"])\n",
      "    27                                           \n",
      "    28                                               # Seleccionar y agrupar data a usar\n",
      "    29         1        410.0    410.0      0.0      data_group = (\n",
      "    30         1    7546235.0    8e+06      0.1          data[[\"date_part\", \"username\", \"id\"]]\n",
      "    31         1     378407.0 378407.0      0.0          .groupby([\"date_part\", \"username\"])\n",
      "    32         1   80445916.0    8e+07      0.8          .count()\n",
      "    33         1    6764604.0    7e+06      0.1          .reset_index()\n",
      "    34         1    3673807.0    4e+06      0.0          .sort_values(by=\"id\", ascending=False)\n",
      "    35                                               )\n",
      "    36                                           \n",
      "    37                                               # Calcular resultados, se obtendra por un lado los dias mas activos y por el otro los usuarios mas activos por cada dÃ­a.\n",
      "    38                                               # Finalmente se realizara el cruce de los 2\n",
      "    39         1        141.0    141.0      0.0      most_active_dates = (\n",
      "    40         1     258353.0 258353.0      0.0          data_group.groupby(\"date_part\")[\"id\"]\n",
      "    41         1    2943021.0    3e+06      0.0          .sum()\n",
      "    42         1     191758.0 191758.0      0.0          .sort_values(ascending=False)\n",
      "    43         1      60673.0  60673.0      0.0          .head(10)\n",
      "    44         1       1442.0   1442.0      0.0          .index\n",
      "    45                                               )\n",
      "    46         1        240.0    240.0      0.0      most_active_user_by_date = (\n",
      "    47         1     237143.0 237143.0      0.0          data_group.groupby([\"date_part\", \"username\"])[\"id\"]\n",
      "    48         1   43465724.0    4e+07      0.4          .sum()\n",
      "    49         1     759870.0 759870.0      0.0          .sort_values(ascending=False)\n",
      "    50         1     383507.0 383507.0      0.0          .groupby(level=0)\n",
      "    51         1   22271580.0    2e+07      0.2          .idxmax()\n",
      "    52         1     324046.0 324046.0      0.0          .apply(lambda x: x[1])\n",
      "    53                                               )\n",
      "    54         2     452735.0 226367.5      0.0      most_active_dates_with_most_active_users = most_active_user_by_date.loc[\n",
      "    55         1        170.0    170.0      0.0          most_active_dates\n",
      "    56                                               ]\n",
      "    57                                           \n",
      "    58                                               # Dar formato final\n",
      "    59         1        230.0    230.0      0.0      out = []\n",
      "    60        11    1554785.0 141344.1      0.0      for index, row in most_active_dates_with_most_active_users.reset_index().iterrows():\n",
      "    61        10     431774.0  43177.4      0.0          out.append((row.iloc[0], row.iloc[1]))\n",
      "    62                                           \n",
      "    63         1        161.0    161.0      0.0      return out"
     ]
    }
   ],
   "source": [
    "%lprun -f q1_time q1_time(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/sandbox/files/latam-challenge/src/q1_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     8    268.5 MiB    268.5 MiB           1   def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "     9                                         \n",
      "    10    268.5 MiB      0.0 MiB           2       def read_json(file_path):\n",
      "    11    268.5 MiB      0.0 MiB           1           data = []\n",
      "    12   1188.1 MiB      0.0 MiB           2           with open(file_path, \"r\") as f:\n",
      "    13                                                     # En el archivo hay un json por linea, hay que itearr y extrar cada uno\n",
      "    14   1188.1 MiB     21.4 MiB      117408               for line in f:\n",
      "    15   1188.1 MiB    898.2 MiB      117407                   data.append(json.loads(line))\n",
      "    16   1188.1 MiB      0.0 MiB           1               f.close()\n",
      "    17                                         \n",
      "    18   1188.1 MiB      0.0 MiB           1           return data\n",
      "    19                                         \n",
      "    20                                             # Leer json y pasar a DF\n",
      "    21   1188.1 MiB      0.0 MiB           1       data = read_json(file_path=file_path)\n",
      "    22   1270.5 MiB     82.5 MiB           1       data = pd.DataFrame.from_records(data)\n",
      "    23                                         \n",
      "    24                                             # Arreglar datos a utilizar\n",
      "    25   1279.5 MiB      9.0 MiB      234815       data[\"date_part\"] = data.date.apply(lambda x: pendulum.parse(x).date())\n",
      "    26   1279.5 MiB      0.0 MiB      234815       data[\"username\"] = data.user.apply(lambda x: x[\"username\"])\n",
      "    27                                         \n",
      "    28                                             # Seleccionar y agrupar data a usar\n",
      "    29   1279.5 MiB      0.0 MiB           1       data_group = (\n",
      "    30   1279.5 MiB      0.0 MiB           1           data[[\"date_part\", \"username\", \"id\"]]\n",
      "    31   1279.5 MiB      0.0 MiB           1           .groupby([\"date_part\", \"username\"])\n",
      "    32   1279.5 MiB      0.0 MiB           1           .count()\n",
      "    33   1279.5 MiB      0.0 MiB           1           .reset_index()\n",
      "    34   1279.5 MiB      0.0 MiB           1           .sort_values(by=\"id\", ascending=False)\n",
      "    35                                             )\n",
      "    36                                         \n",
      "    37                                             # Calcular resultados, se obtendra por un lado los dias mas activos y por el otro los usuarios mas activos por cada dÃ­a.\n",
      "    38                                             # Finalmente se realizara el cruce de los 2\n",
      "    39   1279.5 MiB      0.0 MiB           1       most_active_dates = (\n",
      "    40   1279.5 MiB      0.0 MiB           1           data_group.groupby(\"date_part\")[\"id\"]\n",
      "    41   1279.5 MiB      0.0 MiB           1           .sum()\n",
      "    42   1279.5 MiB      0.0 MiB           1           .sort_values(ascending=False)\n",
      "    43   1279.5 MiB      0.0 MiB           1           .head(10)\n",
      "    44   1279.5 MiB      0.0 MiB           1           .index\n",
      "    45                                             )\n",
      "    46   1279.5 MiB      0.0 MiB           1       most_active_user_by_date = (\n",
      "    47   1279.5 MiB      0.0 MiB           1           data_group.groupby([\"date_part\", \"username\"])[\"id\"]\n",
      "    48   1279.5 MiB      0.0 MiB           1           .sum()\n",
      "    49   1279.5 MiB      0.0 MiB           1           .sort_values(ascending=False)\n",
      "    50   1279.5 MiB      0.0 MiB           1           .groupby(level=0)\n",
      "    51   1279.5 MiB      0.0 MiB           1           .idxmax()\n",
      "    52   1279.5 MiB      0.0 MiB          27           .apply(lambda x: x[1])\n",
      "    53                                             )\n",
      "    54   1279.5 MiB      0.0 MiB           2       most_active_dates_with_most_active_users = most_active_user_by_date.loc[\n",
      "    55   1279.5 MiB      0.0 MiB           1           most_active_dates\n",
      "    56                                             ]\n",
      "    57                                         \n",
      "    58                                             # Dar formato final\n",
      "    59   1279.5 MiB      0.0 MiB           1       out = []\n",
      "    60   1279.5 MiB      0.0 MiB          11       for index, row in most_active_dates_with_most_active_users.reset_index().iterrows():\n",
      "    61   1279.5 MiB      0.0 MiB          10           out.append((row.iloc[0], row.iloc[1]))\n",
      "    62                                         \n",
      "    63   1279.5 MiB      0.0 MiB           1       return out"
     ]
    }
   ],
   "source": [
    "%mprun -f q1_time q1_time(file_path=file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mem pregunta 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q1_memory import  q1_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/10/13 22:35:08 WARN Utils: Your hostname, pescara-pc resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/10/13 22:35:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/13 22:35:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 15.2764 s\n",
      "File: /home/sandbox/files/latam-challenge/src/q1_memory.py\n",
      "Function: q1_memory at line 44\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    44                                           def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    45                                           \n",
      "    46         1 2976992294.0    3e+09     19.5      spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
      "    47                                           \n",
      "    48         1 5282421951.0    5e+09     34.6      data = spark.read.json(file_path)\n",
      "    49         1  443077759.0    4e+08      2.9      dataframe = get_dates_and_users(spark, data)\n",
      "    50         1        481.0    481.0      0.0      del data\n",
      "    51                                           \n",
      "    52         2      15108.0   7554.0      0.0      output = [\n",
      "    53                                                   tuple(row)\n",
      "    54         1 5587778686.0    6e+09     36.6          for row in dataframe.select([col(\"date_part\"), col(\"username\")]).collect()\n",
      "    55                                               ]\n",
      "    56                                           \n",
      "    57         1  986108445.0    1e+09      6.5      spark.stop()\n",
      "    58                                           \n",
      "    59         1        341.0    341.0      0.0      return output"
     ]
    }
   ],
   "source": [
    "%lprun -f q1_memory q1_memory(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 4.8001 s\n",
      "File: /home/sandbox/files/latam-challenge/src/q1_memory.py\n",
      "Function: q1_memory at line 44\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    44                                           def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    45                                           \n",
      "    46         1   89298043.0    9e+07      1.9      spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
      "    47                                           \n",
      "    48         1  828416290.0    8e+08     17.3      data = spark.read.json(file_path)\n",
      "    49         1  105950756.0    1e+08      2.2      dataframe = get_dates_and_users(spark, data)\n",
      "    50         1       2024.0   2024.0      0.0      del data\n",
      "    51                                           \n",
      "    52         2      10339.0   5169.5      0.0      output = [\n",
      "    53                                                   tuple(row)\n",
      "    54         1 2777823350.0    3e+09     57.9          for row in dataframe.select([col(\"date_part\"), col(\"username\")]).collect()\n",
      "    55                                               ]\n",
      "    56                                           \n",
      "    57         1  998598240.0    1e+09     20.8      spark.stop()\n",
      "    58                                           \n",
      "    59         1        170.0    170.0      0.0      return output"
     ]
    }
   ],
   "source": [
    "%lprun -f q1_memory q1_memory(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/sandbox/files/latam-challenge/src/q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    44    273.6 MiB    273.6 MiB           1   def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    45                                         \n",
      "    46    273.6 MiB      0.0 MiB           1       spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
      "    47                                         \n",
      "    48    273.6 MiB      0.0 MiB           1       data = spark.read.json(file_path)\n",
      "    49    273.6 MiB      0.0 MiB           1       dataframe = get_dates_and_users(spark, data)\n",
      "    50    273.6 MiB      0.0 MiB           1       del data\n",
      "    51                                         \n",
      "    52    273.6 MiB      0.0 MiB          24       output = [\n",
      "    53    273.6 MiB      0.0 MiB          10           tuple(row)\n",
      "    54    273.6 MiB      0.0 MiB          11           for row in dataframe.select([col(\"date_part\"), col(\"username\")]).collect()\n",
      "    55                                             ]\n",
      "    56                                         \n",
      "    57    273.6 MiB      0.0 MiB           1       spark.stop()\n",
      "    58                                         \n",
      "    59    273.6 MiB      0.0 MiB           1       return output"
     ]
    }
   ],
   "source": [
    "%mprun -f q1_memory q1_memory(file_path=file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pregunta 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ğŸ™', 5049),\n",
       " ('ğŸ˜‚', 3072),\n",
       " ('ğŸšœ', 2972),\n",
       " ('ğŸŒ¾', 2182),\n",
       " ('ğŸ‡®ğŸ‡³', 2086),\n",
       " ('ğŸ¤£', 1668),\n",
       " ('âœŠ', 1651),\n",
       " ('â¤ï¸', 1382),\n",
       " ('ğŸ™ğŸ»', 1317),\n",
       " ('ğŸ’š', 1040)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from q2_time import  q2_time\n",
    "\n",
    "q2_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ğŸ™', 5049),\n",
       " ('ğŸ˜‚', 3072),\n",
       " ('ğŸšœ', 2972),\n",
       " ('ğŸŒ¾', 2182),\n",
       " ('ğŸ‡®ğŸ‡³', 2086),\n",
       " ('ğŸ¤£', 1668),\n",
       " ('âœŠ', 1651),\n",
       " ('â¤ï¸', 1382),\n",
       " ('ğŸ™ğŸ»', 1317),\n",
       " ('ğŸ’š', 1040)]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from q2_memory import  q2_memory\n",
    "\n",
    "q2_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
