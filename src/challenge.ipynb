{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu solución y todas las suposiciones que estás considerando. Aquí puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pregunta 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se tomaron dos enfoques distintos. El primero consistió en leer los datos con la librería JSON y luego trabajar la información con Pandas. Para el segundo enfoque se utilizó PySpark para la lectura y tratamiento de los datos.\n",
    "\n",
    "En cuanto al tiempo de ejecución con el primer método, se observó que los tiempos eran estables y repetibles, con una duración de alrededor de 11 segundos. En cambio, el segundo método dependía de forma sustancial del estado de la sesión de Spark. En la primera ejecución consecutiva, los tiempos de ejecución rondaban los 15 segundos, muy distintos a los tiempos de las ejecuciones posteriores, que rondaban entre 5 y 6 segundos.\n",
    "\n",
    "Debido a esto, se tomó la decisión de elegir el primer método para q1_time, únicamente por su estabilidad. Cabe considerar que, en el caso de tener un ambiente de ejecución más estable, el segundo método podría ofrecer mejores resultados a largo plazo.\n",
    "\n",
    "En cuanto al uso de memoria, este fue un análisis más directo. Con el método 1, siempre se observó un uso de hasta 1300 MiB, muy por encima del método 2, que rondaba los 300 MiB de forma constante. Por ende, se seleccionó el método 2 para q1_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metodo 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ejecuto multiples veces y siempre se consigui un tiempo de ejecucion de alrededor de 11s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q1_time import  q1_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f q1_time q1_time(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f q1_time q1_time(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mprun -f q1_time q1_time(file_path=file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mem pregunta 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q1_memory import  q1_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f q1_memory q1_memory(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f q1_memory q1_memory(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mprun -f q1_memory q1_memory(file_path=file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pregunta 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este problema se tomo un aproach similar que la pregunta anterior, pero a diferencia de esta, para el tiempo se tomo pyspark y para la memoria funiones convencionales.\n",
    "Dado que solo se tienen que contar ocurrencias a nivel registro, se puede ir iterando fila a fila. \n",
    "\n",
    "Lamentablemente este aproach no obtuvo los resultado esperados y la metodologia sin pyspark utilizo practicamente la misma memoria que la con, y mas en algunas pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/10/14 22:54:59 WARN Utils: Your hostname, pescara-pc resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/10/14 22:54:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/14 22:55:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 18.4042 s\n",
      "File: /home/sandbox/files/latam-challenge/src/q2_time.py\n",
      "Function: q2_time at line 44\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    44                                           def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
      "    45         1 3905414685.0    4e+09     21.2      spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
      "    46                                           \n",
      "    47         1 5956659988.0    6e+09     32.4      data = spark.read.json(file_path)\n",
      "    48         1  335665521.0    3e+08      1.8      dataframe = get_most_used_emojis(spark, data)\n",
      "    49                                           \n",
      "    50         1       1793.0   1793.0      0.0      del data\n",
      "    51                                           \n",
      "    52         2      27091.0  13545.5      0.0      output = [\n",
      "    53         1 7216616653.0    7e+09     39.2          tuple(row) for row in dataframe.select([col(\"emoji\"), col(\"count\")]).collect()\n",
      "    54                                               ]\n",
      "    55                                           \n",
      "    56         1  989797668.0    1e+09      5.4      spark.stop()\n",
      "    57                                           \n",
      "    58         1        291.0    291.0      0.0      return output"
     ]
    }
   ],
   "source": [
    "from q2_time import  q2_time\n",
    "\n",
    "%lprun -f q2_time q2_time(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 8.03239 s\n",
      "File: /home/sandbox/files/latam-challenge/src/q2_time.py\n",
      "Function: q2_time at line 44\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    44                                           def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
      "    45         1   97870683.0    1e+08      1.2      spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
      "    46                                           \n",
      "    47         1  810689662.0    8e+08     10.1      data = spark.read.json(file_path)\n",
      "    48         1  111067536.0    1e+08      1.4      dataframe = get_most_used_emojis(spark, data)\n",
      "    49                                           \n",
      "    50         1        581.0    581.0      0.0      del data\n",
      "    51                                           \n",
      "    52         2       9938.0   4969.0      0.0      output = [\n",
      "    53         1 6016603940.0    6e+09     74.9          tuple(row) for row in dataframe.select([col(\"emoji\"), col(\"count\")]).collect()\n",
      "    54                                               ]\n",
      "    55                                           \n",
      "    56         1  996143456.0    1e+09     12.4      spark.stop()\n",
      "    57                                           \n",
      "    58         1        190.0    190.0      0.0      return output"
     ]
    }
   ],
   "source": [
    "%lprun -f q2_time q2_time(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/sandbox/files/latam-challenge/src/q2_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    44    174.9 MiB    174.9 MiB           1   def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
      "    45    174.9 MiB      0.0 MiB           1       spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
      "    46                                         \n",
      "    47    174.9 MiB      0.0 MiB           1       data = spark.read.json(file_path)\n",
      "    48    174.9 MiB      0.0 MiB           1       dataframe = get_most_used_emojis(spark, data)\n",
      "    49                                         \n",
      "    50    174.9 MiB      0.0 MiB           1       del data\n",
      "    51                                         \n",
      "    52    174.9 MiB      0.0 MiB          24       output = [\n",
      "    53    174.9 MiB      0.0 MiB          11           tuple(row) for row in dataframe.select([col(\"emoji\"), col(\"count\")]).collect()\n",
      "    54                                             ]\n",
      "    55                                         \n",
      "    56    174.9 MiB      0.0 MiB           1       spark.stop()\n",
      "    57                                         \n",
      "    58    174.9 MiB      0.0 MiB           1       return output"
     ]
    }
   ],
   "source": [
    "%mprun -f q2_time q2_time(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** KeyboardInterrupt exception caught in code being profiled."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 9.83701 s\n",
      "File: /home/sandbox/files/latam-challenge/src/q2_memory.py\n",
      "Function: q2_memory at line 7\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     7                                           def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
      "     8         1        912.0    912.0      0.0      result = dict()\n",
      "     9         2     451684.0 225842.0      0.0      with open(file_path, \"r\") as f:\n",
      "    10                                                   # En el archivo hay un json por linea, hay que itearr y extrar cada uno\n",
      "    11      3080   55925861.0  18157.7      0.6          for line in f:\n",
      "    12      3080 8193672422.0    3e+06     83.3              data = pd.json_normalize(json.loads(line))\n",
      "    13      6158  311252272.0  50544.4      3.2              for c in data.content.to_list():\n",
      "    14      4139 1274725765.0 307979.2     13.0                  for e in emoji.emoji_list(c):\n",
      "    15      1060     983372.0    927.7      0.0                      result[e[\"emoji\"]] = result.get(e[\"emoji\"], 0) + 1\n",
      "    16                                           \n",
      "    17                                           \n",
      "    18                                               top_emojis = pd.json_normalize(result).T.reset_index().sort_values(0,ascending=False).head(10)\n",
      "    19                                                   \n",
      "    20                                               out = []\n",
      "    21                                               for index, row in top_emojis.iterrows():\n",
      "    22                                                   out.append((row.loc[\"index\"], row.loc[0]))\n",
      "    23                                           \n",
      "    24                                               return out"
     ]
    }
   ],
   "source": [
    "from q2_memory import  q2_memory\n",
    "\n",
    "%lprun -f q2_memory q2_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mprun -f q2_memory q2_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
