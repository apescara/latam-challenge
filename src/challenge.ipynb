{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu soluciÃ³n y todas las suposiciones que estÃ¡s considerando. AquÃ­ puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pregunta 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se tomaron dos enfoques distintos. El primero consistiÃ³ en leer los datos con la librerÃ­a JSON y luego trabajar la informaciÃ³n con Pandas. Para el segundo enfoque se utilizÃ³ PySpark para la lectura y tratamiento de los datos.\n",
    "\n",
    "En cuanto al tiempo de ejecuciÃ³n con el primer mÃ©todo, se observÃ³ que los tiempos eran estables y repetibles, con una duraciÃ³n de alrededor de 11 segundos. En cambio, el segundo mÃ©todo dependÃ­a de forma sustancial del estado de la sesiÃ³n de Spark. En la primera ejecuciÃ³n consecutiva, los tiempos de ejecuciÃ³n rondaban los 15 segundos, muy distintos a los tiempos de las ejecuciones posteriores, que rondaban entre 5 y 6 segundos.\n",
    "\n",
    "Debido a esto, se tomÃ³ la decisiÃ³n de elegir el primer mÃ©todo para q1_time, Ãºnicamente por su estabilidad. Cabe considerar que, en el caso de tener un ambiente de ejecuciÃ³n mÃ¡s estable, el segundo mÃ©todo podrÃ­a ofrecer mejores resultados a largo plazo.\n",
    "\n",
    "En cuanto al uso de memoria, este fue un anÃ¡lisis mÃ¡s directo. Con el mÃ©todo 1, siempre se observÃ³ un uso de hasta 1300 MiB, muy por encima del mÃ©todo 2, que rondaba los 300 MiB de forma constante. Por ende, se seleccionÃ³ el mÃ©todo 2 para q1_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metodo 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ejecuto multiples veces y siempre se consigui un tiempo de ejecucion de alrededor de 11s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q1_time import  q1_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 11.9141 s\n",
      "File: /home/sandbox/files/latam-challenge/src/q1_time.py\n",
      "Function: q1_time at line 8\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     8                                           def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "     9                                           \n",
      "    10         1       1803.0   1803.0      0.0      def read_json(file_path):\n",
      "    11                                                   data = []\n",
      "    12                                                   with open(file_path, \"r\") as f:\n",
      "    13                                                       # En el archivo hay un json por linea, hay que itearr y extrar cada uno\n",
      "    14                                                       for line in f:\n",
      "    15                                                           data.append(json.loads(line))\n",
      "    16                                                       f.close()\n",
      "    17                                           \n",
      "    18                                                   return data\n",
      "    19                                           \n",
      "    20                                               # Leer json y pasar a DF\n",
      "    21         1 7323865291.0    7e+09     61.5      data = read_json(file_path=file_path)\n",
      "    22         1  828446742.0    8e+08      7.0      data = pd.DataFrame.from_records(data)\n",
      "    23                                           \n",
      "    24                                               # Arreglar datos a utilizar\n",
      "    25         1 3440535343.0    3e+09     28.9      data[\"date_part\"] = data.date.apply(lambda x: pendulum.parse(x).date())\n",
      "    26         1   68738761.0    7e+07      0.6      data[\"username\"] = data.user.apply(lambda x: x[\"username\"])\n",
      "    27                                           \n",
      "    28                                               # Seleccionar y agrupar data a usar\n",
      "    29         1        882.0    882.0      0.0      data_group = (\n",
      "    30         1   10664270.0    1e+07      0.1          data[[\"date_part\", \"username\", \"id\"]]\n",
      "    31         1     393866.0 393866.0      0.0          .groupby([\"date_part\", \"username\"])\n",
      "    32         1   99591728.0    1e+08      0.8          .count()\n",
      "    33         1    8912816.0    9e+06      0.1          .reset_index()\n",
      "    34         1   11243112.0    1e+07      0.1          .sort_values(by=\"id\", ascending=False)\n",
      "    35                                               )\n",
      "    36                                           \n",
      "    37                                               # Calcular resultados, se obtendra por un lado los dias mas activos y por el otro los usuarios mas activos por cada dÃ­a.\n",
      "    38                                               # Finalmente se realizara el cruce de los 2\n",
      "    39         1        120.0    120.0      0.0      most_active_dates = (\n",
      "    40         1     623916.0 623916.0      0.0          data_group.groupby(\"date_part\")[\"id\"]\n",
      "    41         1    7922546.0    8e+06      0.1          .sum()\n",
      "    42         1     295953.0 295953.0      0.0          .sort_values(ascending=False)\n",
      "    43         1     102752.0 102752.0      0.0          .head(10)\n",
      "    44         1       1363.0   1363.0      0.0          .index\n",
      "    45                                               )\n",
      "    46         1        140.0    140.0      0.0      most_active_user_by_date = (\n",
      "    47         1     258864.0 258864.0      0.0          data_group.groupby([\"date_part\", \"username\"])[\"id\"]\n",
      "    48         1   72315035.0    7e+07      0.6          .sum()\n",
      "    49         1     784506.0 784506.0      0.0          .sort_values(ascending=False)\n",
      "    50         1     409745.0 409745.0      0.0          .groupby(level=0)\n",
      "    51         1   36358968.0    4e+07      0.3          .idxmax()\n",
      "    52         1     369620.0 369620.0      0.0          .apply(lambda x: x[1])\n",
      "    53                                               )\n",
      "    54         2     544578.0 272289.0      0.0      most_active_dates_with_most_active_users = most_active_user_by_date.loc[\n",
      "    55         1        190.0    190.0      0.0          most_active_dates\n",
      "    56                                               ]\n",
      "    57                                           \n",
      "    58                                               # Dar formato final\n",
      "    59         1        251.0    251.0      0.0      out = []\n",
      "    60        11    1401589.0 127417.2      0.0      for index, row in most_active_dates_with_most_active_users.reset_index().iterrows():\n",
      "    61        10     347319.0  34731.9      0.0          out.append((row.iloc[0], row.iloc[1]))\n",
      "    62                                           \n",
      "    63         1        140.0    140.0      0.0      return out"
     ]
    }
   ],
   "source": [
    "%lprun -f q1_time q1_time(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 10.4266 s\n",
      "File: /home/sandbox/files/latam-challenge/src/q1_time.py\n",
      "Function: q1_time at line 8\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     8                                           def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "     9                                           \n",
      "    10         1       1262.0   1262.0      0.0      def read_json(file_path):\n",
      "    11                                                   data = []\n",
      "    12                                                   with open(file_path, \"r\") as f:\n",
      "    13                                                       # En el archivo hay un json por linea, hay que itearr y extrar cada uno\n",
      "    14                                                       for line in f:\n",
      "    15                                                           data.append(json.loads(line))\n",
      "    16                                                       f.close()\n",
      "    17                                           \n",
      "    18                                                   return data\n",
      "    19                                           \n",
      "    20                                               # Leer json y pasar a DF\n",
      "    21         1 6426041609.0    6e+09     61.6      data = read_json(file_path=file_path)\n",
      "    22         1  806161848.0    8e+08      7.7      data = pd.DataFrame.from_records(data)\n",
      "    23                                           \n",
      "    24                                               # Arreglar datos a utilizar\n",
      "    25         1 2946161607.0    3e+09     28.3      data[\"date_part\"] = data.date.apply(lambda x: pendulum.parse(x).date())\n",
      "    26         1   69709146.0    7e+07      0.7      data[\"username\"] = data.user.apply(lambda x: x[\"username\"])\n",
      "    27                                           \n",
      "    28                                               # Seleccionar y agrupar data a usar\n",
      "    29         1        351.0    351.0      0.0      data_group = (\n",
      "    30         1    7030088.0    7e+06      0.1          data[[\"date_part\", \"username\", \"id\"]]\n",
      "    31         1     346698.0 346698.0      0.0          .groupby([\"date_part\", \"username\"])\n",
      "    32         1   89284626.0    9e+07      0.9          .count()\n",
      "    33         1    6050829.0    6e+06      0.1          .reset_index()\n",
      "    34         1    5851466.0    6e+06      0.1          .sort_values(by=\"id\", ascending=False)\n",
      "    35                                               )\n",
      "    36                                           \n",
      "    37                                               # Calcular resultados, se obtendra por un lado los dias mas activos y por el otro los usuarios mas activos por cada dÃ­a.\n",
      "    38                                               # Finalmente se realizara el cruce de los 2\n",
      "    39         1        161.0    161.0      0.0      most_active_dates = (\n",
      "    40         1     301052.0 301052.0      0.0          data_group.groupby(\"date_part\")[\"id\"]\n",
      "    41         1    3112096.0    3e+06      0.0          .sum()\n",
      "    42         1     187130.0 187130.0      0.0          .sort_values(ascending=False)\n",
      "    43         1      62377.0  62377.0      0.0          .head(10)\n",
      "    44         1       1212.0   1212.0      0.0          .index\n",
      "    45                                               )\n",
      "    46         1        141.0    141.0      0.0      most_active_user_by_date = (\n",
      "    47         1     191948.0 191948.0      0.0          data_group.groupby([\"date_part\", \"username\"])[\"id\"]\n",
      "    48         1   40863748.0    4e+07      0.4          .sum()\n",
      "    49         1     858164.0 858164.0      0.0          .sort_values(ascending=False)\n",
      "    50         1     385651.0 385651.0      0.0          .groupby(level=0)\n",
      "    51         1   21628460.0    2e+07      0.2          .idxmax()\n",
      "    52         1     304069.0 304069.0      0.0          .apply(lambda x: x[1])\n",
      "    53                                               )\n",
      "    54         2     396512.0 198256.0      0.0      most_active_dates_with_most_active_users = most_active_user_by_date.loc[\n",
      "    55         1        121.0    121.0      0.0          most_active_dates\n",
      "    56                                               ]\n",
      "    57                                           \n",
      "    58                                               # Dar formato final\n",
      "    59         1        210.0    210.0      0.0      out = []\n",
      "    60        11    1308895.0 118990.5      0.0      for index, row in most_active_dates_with_most_active_users.reset_index().iterrows():\n",
      "    61        10     331760.0  33176.0      0.0          out.append((row.iloc[0], row.iloc[1]))\n",
      "    62                                           \n",
      "    63         1        151.0    151.0      0.0      return out"
     ]
    }
   ],
   "source": [
    "%lprun -f q1_time q1_time(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/sandbox/files/latam-challenge/src/q1_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     8    320.5 MiB    320.5 MiB           1   def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "     9                                         \n",
      "    10    320.5 MiB      0.0 MiB           2       def read_json(file_path):\n",
      "    11    320.5 MiB      0.0 MiB           1           data = []\n",
      "    12   1238.1 MiB      0.0 MiB           2           with open(file_path, \"r\") as f:\n",
      "    13                                                     # En el archivo hay un json por linea, hay que itearr y extrar cada uno\n",
      "    14   1238.1 MiB     20.9 MiB      117408               for line in f:\n",
      "    15   1238.1 MiB    896.7 MiB      117407                   data.append(json.loads(line))\n",
      "    16   1238.1 MiB      0.0 MiB           1               f.close()\n",
      "    17                                         \n",
      "    18   1238.1 MiB      0.0 MiB           1           return data\n",
      "    19                                         \n",
      "    20                                             # Leer json y pasar a DF\n",
      "    21   1238.1 MiB      0.0 MiB           1       data = read_json(file_path=file_path)\n",
      "    22   1320.3 MiB     82.3 MiB           1       data = pd.DataFrame.from_records(data)\n",
      "    23                                         \n",
      "    24                                             # Arreglar datos a utilizar\n",
      "    25   1329.2 MiB      8.9 MiB      234815       data[\"date_part\"] = data.date.apply(lambda x: pendulum.parse(x).date())\n",
      "    26   1329.2 MiB      0.0 MiB      234815       data[\"username\"] = data.user.apply(lambda x: x[\"username\"])\n",
      "    27                                         \n",
      "    28                                             # Seleccionar y agrupar data a usar\n",
      "    29   1329.2 MiB      0.0 MiB           1       data_group = (\n",
      "    30   1329.2 MiB      0.0 MiB           1           data[[\"date_part\", \"username\", \"id\"]]\n",
      "    31   1329.2 MiB      0.0 MiB           1           .groupby([\"date_part\", \"username\"])\n",
      "    32   1329.2 MiB      0.0 MiB           1           .count()\n",
      "    33   1329.2 MiB      0.0 MiB           1           .reset_index()\n",
      "    34   1329.2 MiB      0.0 MiB           1           .sort_values(by=\"id\", ascending=False)\n",
      "    35                                             )\n",
      "    36                                         \n",
      "    37                                             # Calcular resultados, se obtendra por un lado los dias mas activos y por el otro los usuarios mas activos por cada dÃ­a.\n",
      "    38                                             # Finalmente se realizara el cruce de los 2\n",
      "    39   1329.2 MiB      0.0 MiB           1       most_active_dates = (\n",
      "    40   1329.2 MiB      0.0 MiB           1           data_group.groupby(\"date_part\")[\"id\"]\n",
      "    41   1329.2 MiB      0.0 MiB           1           .sum()\n",
      "    42   1329.2 MiB      0.0 MiB           1           .sort_values(ascending=False)\n",
      "    43   1329.2 MiB      0.0 MiB           1           .head(10)\n",
      "    44   1329.2 MiB      0.0 MiB           1           .index\n",
      "    45                                             )\n",
      "    46   1329.2 MiB      0.0 MiB           1       most_active_user_by_date = (\n",
      "    47   1329.2 MiB      0.0 MiB           1           data_group.groupby([\"date_part\", \"username\"])[\"id\"]\n",
      "    48   1329.2 MiB      0.0 MiB           1           .sum()\n",
      "    49   1329.2 MiB      0.0 MiB           1           .sort_values(ascending=False)\n",
      "    50   1329.2 MiB      0.0 MiB           1           .groupby(level=0)\n",
      "    51   1329.2 MiB      0.0 MiB           1           .idxmax()\n",
      "    52   1329.2 MiB      0.0 MiB          27           .apply(lambda x: x[1])\n",
      "    53                                             )\n",
      "    54   1329.2 MiB      0.0 MiB           2       most_active_dates_with_most_active_users = most_active_user_by_date.loc[\n",
      "    55   1329.2 MiB      0.0 MiB           1           most_active_dates\n",
      "    56                                             ]\n",
      "    57                                         \n",
      "    58                                             # Dar formato final\n",
      "    59   1329.2 MiB      0.0 MiB           1       out = []\n",
      "    60   1329.2 MiB      0.0 MiB          11       for index, row in most_active_dates_with_most_active_users.reset_index().iterrows():\n",
      "    61   1329.2 MiB      0.0 MiB          10           out.append((row.iloc[0], row.iloc[1]))\n",
      "    62                                         \n",
      "    63   1329.2 MiB      0.0 MiB           1       return out"
     ]
    }
   ],
   "source": [
    "%mprun -f q1_time q1_time(file_path=file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mem pregunta 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q1_memory import  q1_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/10/14 23:56:46 WARN Utils: Your hostname, pescara-pc resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/10/14 23:56:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/14 23:56:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 16.0383 s\n",
      "File: /home/sandbox/files/latam-challenge/src/q1_memory.py\n",
      "Function: q1_memory at line 44\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    44                                           def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    45                                           \n",
      "    46         1 3573701868.0    4e+09     22.3      spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
      "    47                                           \n",
      "    48         1 5591936868.0    6e+09     34.9      data = spark.read.json(file_path)\n",
      "    49         1  614141819.0    6e+08      3.8      dataframe = get_dates_and_users(spark, data)\n",
      "    50         1       1603.0   1603.0      0.0      del data\n",
      "    51                                           \n",
      "    52         2      20819.0  10409.5      0.0      output = [\n",
      "    53                                                   tuple(row)\n",
      "    54         1 5271904276.0    5e+09     32.9          for row in dataframe.select([col(\"date_part\"), col(\"username\")]).collect()\n",
      "    55                                               ]\n",
      "    56                                           \n",
      "    57         1  986639847.0    1e+09      6.2      spark.stop()\n",
      "    58                                           \n",
      "    59         1        481.0    481.0      0.0      return output"
     ]
    }
   ],
   "source": [
    "%lprun -f q1_memory q1_memory(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 5.33392 s\n",
      "File: /home/sandbox/files/latam-challenge/src/q1_memory.py\n",
      "Function: q1_memory at line 44\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    44                                           def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    45                                           \n",
      "    46         1  108041663.0    1e+08      2.0      spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
      "    47                                           \n",
      "    48         1  919476300.0    9e+08     17.2      data = spark.read.json(file_path)\n",
      "    49         1  143086115.0    1e+08      2.7      dataframe = get_dates_and_users(spark, data)\n",
      "    50         1        481.0    481.0      0.0      del data\n",
      "    51                                           \n",
      "    52         2      10129.0   5064.5      0.0      output = [\n",
      "    53                                                   tuple(row)\n",
      "    54         1 3165196143.0    3e+09     59.3          for row in dataframe.select([col(\"date_part\"), col(\"username\")]).collect()\n",
      "    55                                               ]\n",
      "    56                                           \n",
      "    57         1  998112218.0    1e+09     18.7      spark.stop()\n",
      "    58                                           \n",
      "    59         1        260.0    260.0      0.0      return output"
     ]
    }
   ],
   "source": [
    "%lprun -f q1_memory q1_memory(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/sandbox/files/latam-challenge/src/q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    44    307.9 MiB    307.9 MiB           1   def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    45                                         \n",
      "    46    307.9 MiB      0.0 MiB           1       spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
      "    47                                         \n",
      "    48    307.9 MiB      0.0 MiB           1       data = spark.read.json(file_path)\n",
      "    49    307.9 MiB      0.0 MiB           1       dataframe = get_dates_and_users(spark, data)\n",
      "    50    307.9 MiB      0.0 MiB           1       del data\n",
      "    51                                         \n",
      "    52    307.9 MiB      0.0 MiB          24       output = [\n",
      "    53    307.9 MiB      0.0 MiB          10           tuple(row)\n",
      "    54    307.9 MiB      0.0 MiB          11           for row in dataframe.select([col(\"date_part\"), col(\"username\")]).collect()\n",
      "    55                                             ]\n",
      "    56                                         \n",
      "    57    307.9 MiB      0.0 MiB           1       spark.stop()\n",
      "    58                                         \n",
      "    59    307.9 MiB      0.0 MiB           1       return output"
     ]
    }
   ],
   "source": [
    "%mprun -f q1_memory q1_memory(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (Date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (Date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (Date(2021, 2, 16), 'jot__b'),\n",
       " (Date(2021, 2, 14), 'rebelpacifist'),\n",
       " (Date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (Date(2021, 2, 15), 'jot__b'),\n",
       " (Date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (Date(2021, 2, 23), 'Surrypuria'),\n",
       " (Date(2021, 2, 19), 'Preetm91')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_time(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_memory(file_path=file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pregunta 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este problema se tomÃ³ un enfoque similar al de la pregunta anterior, pero a diferencia de esta, para el tiempo se utilizÃ³ PySpark y para la memoria, funciones convencionales.\n",
    "\n",
    "Dado que solo es necesario contar ocurrencias a nivel de registro, se puede iterar fila por fila.\n",
    "\n",
    "Lamentablemente, este enfoque no obtuvo los resultados esperados, y la metodologÃ­a sin PySpark utilizÃ³ prÃ¡cticamente la misma cantidad de memoria que la que usaba PySpark, e incluso mÃ¡s en algunas pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 7.70576 s\n",
      "File: /home/sandbox/files/latam-challenge/src/q2_time.py\n",
      "Function: q2_time at line 44\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    44                                           def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
      "    45         1   84369551.0    8e+07      1.1      spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
      "    46                                           \n",
      "    47         1  710520069.0    7e+08      9.2      data = spark.read.json(file_path)\n",
      "    48         1   82613058.0    8e+07      1.1      dataframe = get_most_used_emojis(spark, data)\n",
      "    49                                           \n",
      "    50         1        872.0    872.0      0.0      del data\n",
      "    51                                           \n",
      "    52         2      11281.0   5640.5      0.0      output = [\n",
      "    53         1 5829209848.0    6e+09     75.6          tuple(row) for row in dataframe.select([col(\"emoji\"), col(\"count\")]).collect()\n",
      "    54                                               ]\n",
      "    55                                           \n",
      "    56         1  999038182.0    1e+09     13.0      spark.stop()\n",
      "    57                                           \n",
      "    58         1        250.0    250.0      0.0      return output"
     ]
    }
   ],
   "source": [
    "from q2_time import  q2_time\n",
    "\n",
    "%lprun -f q2_time q2_time(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/sandbox/files/latam-challenge/src/q2_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    44    308.1 MiB    308.1 MiB           1   def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
      "    45    308.1 MiB      0.0 MiB           1       spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
      "    46                                         \n",
      "    47    308.1 MiB      0.0 MiB           1       data = spark.read.json(file_path)\n",
      "    48    308.1 MiB      0.0 MiB           1       dataframe = get_most_used_emojis(spark, data)\n",
      "    49                                         \n",
      "    50    308.1 MiB      0.0 MiB           1       del data\n",
      "    51                                         \n",
      "    52    308.1 MiB      0.0 MiB          24       output = [\n",
      "    53    308.1 MiB      0.0 MiB          11           tuple(row) for row in dataframe.select([col(\"emoji\"), col(\"count\")]).collect()\n",
      "    54                                             ]\n",
      "    55                                         \n",
      "    56    308.1 MiB      0.0 MiB           1       spark.stop()\n",
      "    57                                         \n",
      "    58    308.1 MiB      0.0 MiB           1       return output"
     ]
    }
   ],
   "source": [
    "%mprun -f q2_time q2_time(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 362.589 s\n",
      "File: /home/sandbox/files/latam-challenge/src/q2_memory.py\n",
      "Function: q2_memory at line 7\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     7                                           def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
      "     8         1       1432.0   1432.0      0.0      result = dict()\n",
      "     9         2      53550.0  26775.0      0.0      with open(file_path, \"r\") as f:\n",
      "    10                                                   # En el archivo hay un json por linea, hay que itearr y extrar cada uno\n",
      "    11    117408 2009027070.0  17111.5      0.6          for line in f:\n",
      "    12    117407        3e+11    3e+06     82.9              data = pd.json_normalize(json.loads(line))\n",
      "    13    234814        1e+10  47962.0      3.1              for c in data.content.to_list():\n",
      "    14    160329        5e+10 302821.0     13.4                  for e in emoji.emoji_list(c):\n",
      "    15     42922   42970470.0   1001.1      0.0                      result[e[\"emoji\"]] = result.get(e[\"emoji\"], 0) + 1\n",
      "    16                                           \n",
      "    17                                           \n",
      "    18         1   23726870.0    2e+07      0.0      top_emojis = pd.json_normalize(result).T.reset_index().sort_values(0,ascending=False).head(10)\n",
      "    19                                                   \n",
      "    20         1        240.0    240.0      0.0      out = []\n",
      "    21        11     994519.0  90410.8      0.0      for index, row in top_emojis.iterrows():\n",
      "    22        10     523618.0  52361.8      0.0          out.append((row.loc[\"index\"], row.loc[0]))\n",
      "    23                                           \n",
      "    24         1        130.0    130.0      0.0      return out"
     ]
    }
   ],
   "source": [
    "from q2_memory import  q2_memory\n",
    "\n",
    "%lprun -f q2_memory q2_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/sandbox/files/latam-challenge/src/q2_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     7    308.1 MiB    308.1 MiB           1   def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
      "     8    308.1 MiB      0.0 MiB           1       result = dict()\n",
      "     9    308.1 MiB      0.0 MiB           2       with open(file_path, \"r\") as f:\n",
      "    10                                                 # En el archivo hay un json por linea, hay que itearr y extrar cada uno\n",
      "    11    308.1 MiB      0.0 MiB      117408           for line in f:\n",
      "    12    308.1 MiB      0.0 MiB      117407               data = pd.json_normalize(json.loads(line))\n",
      "    13    308.1 MiB      0.0 MiB      234814               for c in data.content.to_list():\n",
      "    14    308.1 MiB      0.0 MiB      160329                   for e in emoji.emoji_list(c):\n",
      "    15    308.1 MiB      0.0 MiB       42922                       result[e[\"emoji\"]] = result.get(e[\"emoji\"], 0) + 1\n",
      "    16                                         \n",
      "    17                                         \n",
      "    18    308.1 MiB      0.0 MiB           1       top_emojis = pd.json_normalize(result).T.reset_index().sort_values(0,ascending=False).head(10)\n",
      "    19                                                 \n",
      "    20    308.1 MiB      0.0 MiB           1       out = []\n",
      "    21    308.1 MiB      0.0 MiB          11       for index, row in top_emojis.iterrows():\n",
      "    22    308.1 MiB      0.0 MiB          10           out.append((row.loc[\"index\"], row.loc[0]))\n",
      "    23                                         \n",
      "    24    308.1 MiB      0.0 MiB           1       return out"
     ]
    }
   ],
   "source": [
    "%mprun -f q2_memory q2_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ðŸ™', 5049),\n",
       " ('ðŸ˜‚', 3072),\n",
       " ('ðŸšœ', 2972),\n",
       " ('ðŸŒ¾', 2182),\n",
       " ('ðŸ‡®ðŸ‡³', 2086),\n",
       " ('ðŸ¤£', 1668),\n",
       " ('âœŠ', 1651),\n",
       " ('â¤ï¸', 1382),\n",
       " ('ðŸ™ðŸ»', 1317),\n",
       " ('ðŸ’š', 1040)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2_time(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ðŸ™', 5049),\n",
       " ('ðŸ˜‚', 3072),\n",
       " ('ðŸšœ', 2972),\n",
       " ('ðŸŒ¾', 2182),\n",
       " ('ðŸ‡®ðŸ‡³', 2086),\n",
       " ('ðŸ¤£', 1668),\n",
       " ('âœŠ', 1651),\n",
       " ('â¤ï¸', 1382),\n",
       " ('ðŸ™ðŸ»', 1317),\n",
       " ('ðŸ’š', 1040)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pregunta 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ProblemÃ¡tica similar al ejercicio anterior, salvo por la diferencia de que el dato venÃ­a dentro de una lista de JSONs y no en un texto plano (exceptuando el hecho de que se tenÃ­an que extraer emojis).\n",
    "\n",
    "Por esta razÃ³n, se tomÃ³ un enfoque similar al de la pregunta anterior, con resultados similares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 2.09702 s\n",
      "File: /home/sandbox/files/latam-challenge/src/q3_time.py\n",
      "Function: q3_time at line 25\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    25                                           def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
      "    26         1  109873134.0    1e+08      5.2      spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
      "    27                                           \n",
      "    28         1  691153959.0    7e+08     33.0      data = spark.read.json(file_path)\n",
      "    29         1   40230284.0    4e+07      1.9      dataframe = get_most_mentions(spark, data)\n",
      "    30                                           \n",
      "    31         1        722.0    722.0      0.0      del data\n",
      "    32                                           \n",
      "    33         2      12412.0   6206.0      0.0      output = [\n",
      "    34         1 1011795270.0    1e+09     48.2          tuple(row) for row in dataframe.select([col(\"username\"), col(\"count\")]).collect()\n",
      "    35                                               ]\n",
      "    36                                           \n",
      "    37         1  243952115.0    2e+08     11.6      spark.stop()\n",
      "    38                                           \n",
      "    39         1        180.0    180.0      0.0      return output"
     ]
    }
   ],
   "source": [
    "from q3_time import  q3_time\n",
    "\n",
    "%lprun -f q3_time q3_time(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/sandbox/files/latam-challenge/src/q3_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    25    308.1 MiB    308.1 MiB           1   def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
      "    26    308.1 MiB      0.0 MiB           1       spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
      "    27                                         \n",
      "    28    308.1 MiB      0.0 MiB           1       data = spark.read.json(file_path)\n",
      "    29    308.1 MiB      0.0 MiB           1       dataframe = get_most_mentions(spark, data)\n",
      "    30                                         \n",
      "    31    308.1 MiB      0.0 MiB           1       del data\n",
      "    32                                         \n",
      "    33    308.1 MiB      0.0 MiB          24       output = [\n",
      "    34    308.1 MiB      0.0 MiB          11           tuple(row) for row in dataframe.select([col(\"username\"), col(\"count\")]).collect()\n",
      "    35                                             ]\n",
      "    36                                         \n",
      "    37    308.1 MiB      0.0 MiB           1       spark.stop()\n",
      "    38                                         \n",
      "    39    308.1 MiB      0.0 MiB           1       return output"
     ]
    }
   ],
   "source": [
    "%mprun -f q3_time q3_time(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 307.037 s\n",
      "File: /home/sandbox/files/latam-challenge/src/q3_memory.py\n",
      "Function: q3_memory at line 6\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     6                                           def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
      "     7         1       2164.0   2164.0      0.0      result = dict()\n",
      "     8         2     136466.0  68233.0      0.0      with open(file_path, \"r\") as f:\n",
      "     9                                                   # En el archivo hay un json por linea, hay que itearr y extrar cada uno\n",
      "    10    117408 1815098362.0  15459.8      0.6          for line in f:\n",
      "    11    117407        3e+11    3e+06     97.3              data = pd.json_normalize(json.loads(line))\n",
      "    12    117407 3005368140.0  25597.9      1.0              if \"quotedTweet.mentionedUsers\" in data.columns: # Hay registros que no tienen la columna\n",
      "    13     41436 3248181457.0  78390.3      1.1                  if data[\"quotedTweet.mentionedUsers\"][0]: # Y otros que vienen con \"None\"\n",
      "    14     23579  137980985.0   5851.9      0.0                      for u in data[\"quotedTweet.mentionedUsers\"][0]:\n",
      "    15     16075   20213130.0   1257.4      0.0                          result[u[\"username\"]] = result.get(u[\"username\"], 0) + 1\n",
      "    16                                               \n",
      "    17         1   76046699.0    8e+07      0.0      top_mentions = pd.json_normalize(result).T.reset_index().sort_values(0,ascending=False).head(10)\n",
      "    18                                                   \n",
      "    19         1        221.0    221.0      0.0      out = []\n",
      "    20        11     764049.0  69459.0      0.0      for index, row in top_mentions.iterrows():\n",
      "    21        10     442548.0  44254.8      0.0          out.append((row.loc[\"index\"], row.loc[0]))\n",
      "    22                                           \n",
      "    23         1        171.0    171.0      0.0      return out"
     ]
    }
   ],
   "source": [
    "from q3_memory import  q3_memory\n",
    "\n",
    "%lprun -f q3_memory q3_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/sandbox/files/latam-challenge/src/q3_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     6    308.2 MiB    308.2 MiB           1   def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
      "     7    308.2 MiB      0.0 MiB           1       result = dict()\n",
      "     8    314.0 MiB      0.0 MiB           2       with open(file_path, \"r\") as f:\n",
      "     9                                                 # En el archivo hay un json por linea, hay que itearr y extrar cada uno\n",
      "    10    314.0 MiB      0.0 MiB      117408           for line in f:\n",
      "    11    314.0 MiB      5.7 MiB      117407               data = pd.json_normalize(json.loads(line))\n",
      "    12    314.0 MiB      0.0 MiB      117407               if \"quotedTweet.mentionedUsers\" in data.columns: # Hay registros que no tienen la columna\n",
      "    13    314.0 MiB      0.0 MiB       41436                   if data[\"quotedTweet.mentionedUsers\"][0]: # Y otros que vienen con \"None\"\n",
      "    14    314.0 MiB      0.0 MiB       23579                       for u in data[\"quotedTweet.mentionedUsers\"][0]:\n",
      "    15    314.0 MiB      0.0 MiB       16075                           result[u[\"username\"]] = result.get(u[\"username\"], 0) + 1\n",
      "    16                                             \n",
      "    17    314.0 MiB      0.0 MiB           1       top_mentions = pd.json_normalize(result).T.reset_index().sort_values(0,ascending=False).head(10)\n",
      "    18                                                 \n",
      "    19    314.0 MiB      0.0 MiB           1       out = []\n",
      "    20    314.0 MiB      0.0 MiB          11       for index, row in top_mentions.iterrows():\n",
      "    21    314.0 MiB      0.0 MiB          10           out.append((row.loc[\"index\"], row.loc[0]))\n",
      "    22                                         \n",
      "    23    314.0 MiB      0.0 MiB           1       return out"
     ]
    }
   ],
   "source": [
    "%mprun -f q3_memory q3_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('narendramodi', 358),\n",
       " ('DelhiPolice', 320),\n",
       " ('nytimes', 312),\n",
       " ('GretaThunberg', 226),\n",
       " ('Kisanektamorcha', 205),\n",
       " ('jazzyb', 204),\n",
       " ('RakeshTikaitBKU', 204),\n",
       " ('mujerxsrising', 173),\n",
       " ('Bkuektaugrahan', 166),\n",
       " ('sushant_says', 157)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3_time(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('narendramodi', 358),\n",
       " ('DelhiPolice', 320),\n",
       " ('nytimes', 312),\n",
       " ('GretaThunberg', 226),\n",
       " ('Kisanektamorcha', 205),\n",
       " ('RakeshTikaitBKU', 204),\n",
       " ('jazzyb', 204),\n",
       " ('mujerxsrising', 173),\n",
       " ('Bkuektaugrahan', 166),\n",
       " ('sushant_says', 157)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3_memory(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
