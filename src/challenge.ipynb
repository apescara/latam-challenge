{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu solución y todas las suposiciones que estás considerando. Aquí puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pregunta 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se tomaron dos enfoques distintos. El primero consistió en leer los datos con la librería JSON y luego trabajar la información con Pandas. Para el segundo enfoque se utilizó PySpark para la lectura y tratamiento de los datos.\n",
    "\n",
    "En cuanto al tiempo de ejecución con el primer método, se observó que los tiempos eran estables y repetibles, con una duración de alrededor de 11 segundos. En cambio, el segundo método dependía de forma sustancial del estado de la sesión de Spark. En la primera ejecución consecutiva, los tiempos de ejecución rondaban los 15 segundos, muy distintos a los tiempos de las ejecuciones posteriores, que rondaban entre 5 y 6 segundos.\n",
    "\n",
    "Debido a esto, se tomó la decisión de elegir el primer método para q1_time, únicamente por su estabilidad. Cabe considerar que, en el caso de tener un ambiente de ejecución más estable, el segundo método podría ofrecer mejores resultados a largo plazo.\n",
    "\n",
    "En cuanto al uso de memoria, este fue un análisis más directo. Con el método 1, siempre se observó un uso de hasta 1300 MiB, muy por encima del método 2, que rondaba los 300 MiB de forma constante. Por ende, se seleccionó el método 2 para q1_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metodo 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ejecuto multiples veces y siempre se consigui un tiempo de ejecucion de alrededor de 11s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q1_time import  q1_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 11.9141 s\n",
      "File: /home/sandbox/files/latam-challenge/src/q1_time.py\n",
      "Function: q1_time at line 8\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     8                                           def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "     9                                           \n",
      "    10         1       1803.0   1803.0      0.0      def read_json(file_path):\n",
      "    11                                                   data = []\n",
      "    12                                                   with open(file_path, \"r\") as f:\n",
      "    13                                                       # En el archivo hay un json por linea, hay que itearr y extrar cada uno\n",
      "    14                                                       for line in f:\n",
      "    15                                                           data.append(json.loads(line))\n",
      "    16                                                       f.close()\n",
      "    17                                           \n",
      "    18                                                   return data\n",
      "    19                                           \n",
      "    20                                               # Leer json y pasar a DF\n",
      "    21         1 7323865291.0    7e+09     61.5      data = read_json(file_path=file_path)\n",
      "    22         1  828446742.0    8e+08      7.0      data = pd.DataFrame.from_records(data)\n",
      "    23                                           \n",
      "    24                                               # Arreglar datos a utilizar\n",
      "    25         1 3440535343.0    3e+09     28.9      data[\"date_part\"] = data.date.apply(lambda x: pendulum.parse(x).date())\n",
      "    26         1   68738761.0    7e+07      0.6      data[\"username\"] = data.user.apply(lambda x: x[\"username\"])\n",
      "    27                                           \n",
      "    28                                               # Seleccionar y agrupar data a usar\n",
      "    29         1        882.0    882.0      0.0      data_group = (\n",
      "    30         1   10664270.0    1e+07      0.1          data[[\"date_part\", \"username\", \"id\"]]\n",
      "    31         1     393866.0 393866.0      0.0          .groupby([\"date_part\", \"username\"])\n",
      "    32         1   99591728.0    1e+08      0.8          .count()\n",
      "    33         1    8912816.0    9e+06      0.1          .reset_index()\n",
      "    34         1   11243112.0    1e+07      0.1          .sort_values(by=\"id\", ascending=False)\n",
      "    35                                               )\n",
      "    36                                           \n",
      "    37                                               # Calcular resultados, se obtendra por un lado los dias mas activos y por el otro los usuarios mas activos por cada día.\n",
      "    38                                               # Finalmente se realizara el cruce de los 2\n",
      "    39         1        120.0    120.0      0.0      most_active_dates = (\n",
      "    40         1     623916.0 623916.0      0.0          data_group.groupby(\"date_part\")[\"id\"]\n",
      "    41         1    7922546.0    8e+06      0.1          .sum()\n",
      "    42         1     295953.0 295953.0      0.0          .sort_values(ascending=False)\n",
      "    43         1     102752.0 102752.0      0.0          .head(10)\n",
      "    44         1       1363.0   1363.0      0.0          .index\n",
      "    45                                               )\n",
      "    46         1        140.0    140.0      0.0      most_active_user_by_date = (\n",
      "    47         1     258864.0 258864.0      0.0          data_group.groupby([\"date_part\", \"username\"])[\"id\"]\n",
      "    48         1   72315035.0    7e+07      0.6          .sum()\n",
      "    49         1     784506.0 784506.0      0.0          .sort_values(ascending=False)\n",
      "    50         1     409745.0 409745.0      0.0          .groupby(level=0)\n",
      "    51         1   36358968.0    4e+07      0.3          .idxmax()\n",
      "    52         1     369620.0 369620.0      0.0          .apply(lambda x: x[1])\n",
      "    53                                               )\n",
      "    54         2     544578.0 272289.0      0.0      most_active_dates_with_most_active_users = most_active_user_by_date.loc[\n",
      "    55         1        190.0    190.0      0.0          most_active_dates\n",
      "    56                                               ]\n",
      "    57                                           \n",
      "    58                                               # Dar formato final\n",
      "    59         1        251.0    251.0      0.0      out = []\n",
      "    60        11    1401589.0 127417.2      0.0      for index, row in most_active_dates_with_most_active_users.reset_index().iterrows():\n",
      "    61        10     347319.0  34731.9      0.0          out.append((row.iloc[0], row.iloc[1]))\n",
      "    62                                           \n",
      "    63         1        140.0    140.0      0.0      return out"
     ]
    }
   ],
   "source": [
    "%lprun -f q1_time q1_time(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 10.4266 s\n",
      "File: /home/sandbox/files/latam-challenge/src/q1_time.py\n",
      "Function: q1_time at line 8\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     8                                           def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "     9                                           \n",
      "    10         1       1262.0   1262.0      0.0      def read_json(file_path):\n",
      "    11                                                   data = []\n",
      "    12                                                   with open(file_path, \"r\") as f:\n",
      "    13                                                       # En el archivo hay un json por linea, hay que itearr y extrar cada uno\n",
      "    14                                                       for line in f:\n",
      "    15                                                           data.append(json.loads(line))\n",
      "    16                                                       f.close()\n",
      "    17                                           \n",
      "    18                                                   return data\n",
      "    19                                           \n",
      "    20                                               # Leer json y pasar a DF\n",
      "    21         1 6426041609.0    6e+09     61.6      data = read_json(file_path=file_path)\n",
      "    22         1  806161848.0    8e+08      7.7      data = pd.DataFrame.from_records(data)\n",
      "    23                                           \n",
      "    24                                               # Arreglar datos a utilizar\n",
      "    25         1 2946161607.0    3e+09     28.3      data[\"date_part\"] = data.date.apply(lambda x: pendulum.parse(x).date())\n",
      "    26         1   69709146.0    7e+07      0.7      data[\"username\"] = data.user.apply(lambda x: x[\"username\"])\n",
      "    27                                           \n",
      "    28                                               # Seleccionar y agrupar data a usar\n",
      "    29         1        351.0    351.0      0.0      data_group = (\n",
      "    30         1    7030088.0    7e+06      0.1          data[[\"date_part\", \"username\", \"id\"]]\n",
      "    31         1     346698.0 346698.0      0.0          .groupby([\"date_part\", \"username\"])\n",
      "    32         1   89284626.0    9e+07      0.9          .count()\n",
      "    33         1    6050829.0    6e+06      0.1          .reset_index()\n",
      "    34         1    5851466.0    6e+06      0.1          .sort_values(by=\"id\", ascending=False)\n",
      "    35                                               )\n",
      "    36                                           \n",
      "    37                                               # Calcular resultados, se obtendra por un lado los dias mas activos y por el otro los usuarios mas activos por cada día.\n",
      "    38                                               # Finalmente se realizara el cruce de los 2\n",
      "    39         1        161.0    161.0      0.0      most_active_dates = (\n",
      "    40         1     301052.0 301052.0      0.0          data_group.groupby(\"date_part\")[\"id\"]\n",
      "    41         1    3112096.0    3e+06      0.0          .sum()\n",
      "    42         1     187130.0 187130.0      0.0          .sort_values(ascending=False)\n",
      "    43         1      62377.0  62377.0      0.0          .head(10)\n",
      "    44         1       1212.0   1212.0      0.0          .index\n",
      "    45                                               )\n",
      "    46         1        141.0    141.0      0.0      most_active_user_by_date = (\n",
      "    47         1     191948.0 191948.0      0.0          data_group.groupby([\"date_part\", \"username\"])[\"id\"]\n",
      "    48         1   40863748.0    4e+07      0.4          .sum()\n",
      "    49         1     858164.0 858164.0      0.0          .sort_values(ascending=False)\n",
      "    50         1     385651.0 385651.0      0.0          .groupby(level=0)\n",
      "    51         1   21628460.0    2e+07      0.2          .idxmax()\n",
      "    52         1     304069.0 304069.0      0.0          .apply(lambda x: x[1])\n",
      "    53                                               )\n",
      "    54         2     396512.0 198256.0      0.0      most_active_dates_with_most_active_users = most_active_user_by_date.loc[\n",
      "    55         1        121.0    121.0      0.0          most_active_dates\n",
      "    56                                               ]\n",
      "    57                                           \n",
      "    58                                               # Dar formato final\n",
      "    59         1        210.0    210.0      0.0      out = []\n",
      "    60        11    1308895.0 118990.5      0.0      for index, row in most_active_dates_with_most_active_users.reset_index().iterrows():\n",
      "    61        10     331760.0  33176.0      0.0          out.append((row.iloc[0], row.iloc[1]))\n",
      "    62                                           \n",
      "    63         1        151.0    151.0      0.0      return out"
     ]
    }
   ],
   "source": [
    "%lprun -f q1_time q1_time(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/sandbox/files/latam-challenge/src/q1_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     8    320.5 MiB    320.5 MiB           1   def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "     9                                         \n",
      "    10    320.5 MiB      0.0 MiB           2       def read_json(file_path):\n",
      "    11    320.5 MiB      0.0 MiB           1           data = []\n",
      "    12   1238.1 MiB      0.0 MiB           2           with open(file_path, \"r\") as f:\n",
      "    13                                                     # En el archivo hay un json por linea, hay que itearr y extrar cada uno\n",
      "    14   1238.1 MiB     20.9 MiB      117408               for line in f:\n",
      "    15   1238.1 MiB    896.7 MiB      117407                   data.append(json.loads(line))\n",
      "    16   1238.1 MiB      0.0 MiB           1               f.close()\n",
      "    17                                         \n",
      "    18   1238.1 MiB      0.0 MiB           1           return data\n",
      "    19                                         \n",
      "    20                                             # Leer json y pasar a DF\n",
      "    21   1238.1 MiB      0.0 MiB           1       data = read_json(file_path=file_path)\n",
      "    22   1320.3 MiB     82.3 MiB           1       data = pd.DataFrame.from_records(data)\n",
      "    23                                         \n",
      "    24                                             # Arreglar datos a utilizar\n",
      "    25   1329.2 MiB      8.9 MiB      234815       data[\"date_part\"] = data.date.apply(lambda x: pendulum.parse(x).date())\n",
      "    26   1329.2 MiB      0.0 MiB      234815       data[\"username\"] = data.user.apply(lambda x: x[\"username\"])\n",
      "    27                                         \n",
      "    28                                             # Seleccionar y agrupar data a usar\n",
      "    29   1329.2 MiB      0.0 MiB           1       data_group = (\n",
      "    30   1329.2 MiB      0.0 MiB           1           data[[\"date_part\", \"username\", \"id\"]]\n",
      "    31   1329.2 MiB      0.0 MiB           1           .groupby([\"date_part\", \"username\"])\n",
      "    32   1329.2 MiB      0.0 MiB           1           .count()\n",
      "    33   1329.2 MiB      0.0 MiB           1           .reset_index()\n",
      "    34   1329.2 MiB      0.0 MiB           1           .sort_values(by=\"id\", ascending=False)\n",
      "    35                                             )\n",
      "    36                                         \n",
      "    37                                             # Calcular resultados, se obtendra por un lado los dias mas activos y por el otro los usuarios mas activos por cada día.\n",
      "    38                                             # Finalmente se realizara el cruce de los 2\n",
      "    39   1329.2 MiB      0.0 MiB           1       most_active_dates = (\n",
      "    40   1329.2 MiB      0.0 MiB           1           data_group.groupby(\"date_part\")[\"id\"]\n",
      "    41   1329.2 MiB      0.0 MiB           1           .sum()\n",
      "    42   1329.2 MiB      0.0 MiB           1           .sort_values(ascending=False)\n",
      "    43   1329.2 MiB      0.0 MiB           1           .head(10)\n",
      "    44   1329.2 MiB      0.0 MiB           1           .index\n",
      "    45                                             )\n",
      "    46   1329.2 MiB      0.0 MiB           1       most_active_user_by_date = (\n",
      "    47   1329.2 MiB      0.0 MiB           1           data_group.groupby([\"date_part\", \"username\"])[\"id\"]\n",
      "    48   1329.2 MiB      0.0 MiB           1           .sum()\n",
      "    49   1329.2 MiB      0.0 MiB           1           .sort_values(ascending=False)\n",
      "    50   1329.2 MiB      0.0 MiB           1           .groupby(level=0)\n",
      "    51   1329.2 MiB      0.0 MiB           1           .idxmax()\n",
      "    52   1329.2 MiB      0.0 MiB          27           .apply(lambda x: x[1])\n",
      "    53                                             )\n",
      "    54   1329.2 MiB      0.0 MiB           2       most_active_dates_with_most_active_users = most_active_user_by_date.loc[\n",
      "    55   1329.2 MiB      0.0 MiB           1           most_active_dates\n",
      "    56                                             ]\n",
      "    57                                         \n",
      "    58                                             # Dar formato final\n",
      "    59   1329.2 MiB      0.0 MiB           1       out = []\n",
      "    60   1329.2 MiB      0.0 MiB          11       for index, row in most_active_dates_with_most_active_users.reset_index().iterrows():\n",
      "    61   1329.2 MiB      0.0 MiB          10           out.append((row.iloc[0], row.iloc[1]))\n",
      "    62                                         \n",
      "    63   1329.2 MiB      0.0 MiB           1       return out"
     ]
    }
   ],
   "source": [
    "%mprun -f q1_time q1_time(file_path=file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mem pregunta 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q1_memory import  q1_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/10/14 23:56:46 WARN Utils: Your hostname, pescara-pc resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/10/14 23:56:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/14 23:56:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 16.0383 s\n",
      "File: /home/sandbox/files/latam-challenge/src/q1_memory.py\n",
      "Function: q1_memory at line 44\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    44                                           def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    45                                           \n",
      "    46         1 3573701868.0    4e+09     22.3      spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
      "    47                                           \n",
      "    48         1 5591936868.0    6e+09     34.9      data = spark.read.json(file_path)\n",
      "    49         1  614141819.0    6e+08      3.8      dataframe = get_dates_and_users(spark, data)\n",
      "    50         1       1603.0   1603.0      0.0      del data\n",
      "    51                                           \n",
      "    52         2      20819.0  10409.5      0.0      output = [\n",
      "    53                                                   tuple(row)\n",
      "    54         1 5271904276.0    5e+09     32.9          for row in dataframe.select([col(\"date_part\"), col(\"username\")]).collect()\n",
      "    55                                               ]\n",
      "    56                                           \n",
      "    57         1  986639847.0    1e+09      6.2      spark.stop()\n",
      "    58                                           \n",
      "    59         1        481.0    481.0      0.0      return output"
     ]
    }
   ],
   "source": [
    "%lprun -f q1_memory q1_memory(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 5.33392 s\n",
      "File: /home/sandbox/files/latam-challenge/src/q1_memory.py\n",
      "Function: q1_memory at line 44\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    44                                           def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    45                                           \n",
      "    46         1  108041663.0    1e+08      2.0      spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
      "    47                                           \n",
      "    48         1  919476300.0    9e+08     17.2      data = spark.read.json(file_path)\n",
      "    49         1  143086115.0    1e+08      2.7      dataframe = get_dates_and_users(spark, data)\n",
      "    50         1        481.0    481.0      0.0      del data\n",
      "    51                                           \n",
      "    52         2      10129.0   5064.5      0.0      output = [\n",
      "    53                                                   tuple(row)\n",
      "    54         1 3165196143.0    3e+09     59.3          for row in dataframe.select([col(\"date_part\"), col(\"username\")]).collect()\n",
      "    55                                               ]\n",
      "    56                                           \n",
      "    57         1  998112218.0    1e+09     18.7      spark.stop()\n",
      "    58                                           \n",
      "    59         1        260.0    260.0      0.0      return output"
     ]
    }
   ],
   "source": [
    "%lprun -f q1_memory q1_memory(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/sandbox/files/latam-challenge/src/q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    44    307.9 MiB    307.9 MiB           1   def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    45                                         \n",
      "    46    307.9 MiB      0.0 MiB           1       spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
      "    47                                         \n",
      "    48    307.9 MiB      0.0 MiB           1       data = spark.read.json(file_path)\n",
      "    49    307.9 MiB      0.0 MiB           1       dataframe = get_dates_and_users(spark, data)\n",
      "    50    307.9 MiB      0.0 MiB           1       del data\n",
      "    51                                         \n",
      "    52    307.9 MiB      0.0 MiB          24       output = [\n",
      "    53    307.9 MiB      0.0 MiB          10           tuple(row)\n",
      "    54    307.9 MiB      0.0 MiB          11           for row in dataframe.select([col(\"date_part\"), col(\"username\")]).collect()\n",
      "    55                                             ]\n",
      "    56                                         \n",
      "    57    307.9 MiB      0.0 MiB           1       spark.stop()\n",
      "    58                                         \n",
      "    59    307.9 MiB      0.0 MiB           1       return output"
     ]
    }
   ],
   "source": [
    "%mprun -f q1_memory q1_memory(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (Date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (Date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (Date(2021, 2, 16), 'jot__b'),\n",
       " (Date(2021, 2, 14), 'rebelpacifist'),\n",
       " (Date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (Date(2021, 2, 15), 'jot__b'),\n",
       " (Date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (Date(2021, 2, 23), 'Surrypuria'),\n",
       " (Date(2021, 2, 19), 'Preetm91')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_time(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n",
       " (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n",
       " (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n",
       " (datetime.date(2021, 2, 16), 'jot__b'),\n",
       " (datetime.date(2021, 2, 14), 'rebelpacifist'),\n",
       " (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n",
       " (datetime.date(2021, 2, 15), 'jot__b'),\n",
       " (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n",
       " (datetime.date(2021, 2, 23), 'Surrypuria'),\n",
       " (datetime.date(2021, 2, 19), 'Preetm91')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_memory(file_path=file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pregunta 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este problema se tomó un enfoque similar al de la pregunta anterior, pero a diferencia de esta, para el tiempo se utilizó PySpark y para la memoria, funciones convencionales.\n",
    "\n",
    "Dado que solo es necesario contar ocurrencias a nivel de registro, se puede iterar fila por fila.\n",
    "\n",
    "Lamentablemente, este enfoque no obtuvo los resultados esperados, y la metodología sin PySpark utilizó prácticamente la misma cantidad de memoria que la que usaba PySpark, e incluso más en algunas pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 7.70576 s\n",
      "File: /home/sandbox/files/latam-challenge/src/q2_time.py\n",
      "Function: q2_time at line 44\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    44                                           def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
      "    45         1   84369551.0    8e+07      1.1      spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
      "    46                                           \n",
      "    47         1  710520069.0    7e+08      9.2      data = spark.read.json(file_path)\n",
      "    48         1   82613058.0    8e+07      1.1      dataframe = get_most_used_emojis(spark, data)\n",
      "    49                                           \n",
      "    50         1        872.0    872.0      0.0      del data\n",
      "    51                                           \n",
      "    52         2      11281.0   5640.5      0.0      output = [\n",
      "    53         1 5829209848.0    6e+09     75.6          tuple(row) for row in dataframe.select([col(\"emoji\"), col(\"count\")]).collect()\n",
      "    54                                               ]\n",
      "    55                                           \n",
      "    56         1  999038182.0    1e+09     13.0      spark.stop()\n",
      "    57                                           \n",
      "    58         1        250.0    250.0      0.0      return output"
     ]
    }
   ],
   "source": [
    "from q2_time import  q2_time\n",
    "\n",
    "%lprun -f q2_time q2_time(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/sandbox/files/latam-challenge/src/q2_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    44    308.1 MiB    308.1 MiB           1   def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
      "    45    308.1 MiB      0.0 MiB           1       spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
      "    46                                         \n",
      "    47    308.1 MiB      0.0 MiB           1       data = spark.read.json(file_path)\n",
      "    48    308.1 MiB      0.0 MiB           1       dataframe = get_most_used_emojis(spark, data)\n",
      "    49                                         \n",
      "    50    308.1 MiB      0.0 MiB           1       del data\n",
      "    51                                         \n",
      "    52    308.1 MiB      0.0 MiB          24       output = [\n",
      "    53    308.1 MiB      0.0 MiB          11           tuple(row) for row in dataframe.select([col(\"emoji\"), col(\"count\")]).collect()\n",
      "    54                                             ]\n",
      "    55                                         \n",
      "    56    308.1 MiB      0.0 MiB           1       spark.stop()\n",
      "    57                                         \n",
      "    58    308.1 MiB      0.0 MiB           1       return output"
     ]
    }
   ],
   "source": [
    "%mprun -f q2_time q2_time(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 362.589 s\n",
      "File: /home/sandbox/files/latam-challenge/src/q2_memory.py\n",
      "Function: q2_memory at line 7\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     7                                           def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
      "     8         1       1432.0   1432.0      0.0      result = dict()\n",
      "     9         2      53550.0  26775.0      0.0      with open(file_path, \"r\") as f:\n",
      "    10                                                   # En el archivo hay un json por linea, hay que itearr y extrar cada uno\n",
      "    11    117408 2009027070.0  17111.5      0.6          for line in f:\n",
      "    12    117407        3e+11    3e+06     82.9              data = pd.json_normalize(json.loads(line))\n",
      "    13    234814        1e+10  47962.0      3.1              for c in data.content.to_list():\n",
      "    14    160329        5e+10 302821.0     13.4                  for e in emoji.emoji_list(c):\n",
      "    15     42922   42970470.0   1001.1      0.0                      result[e[\"emoji\"]] = result.get(e[\"emoji\"], 0) + 1\n",
      "    16                                           \n",
      "    17                                           \n",
      "    18         1   23726870.0    2e+07      0.0      top_emojis = pd.json_normalize(result).T.reset_index().sort_values(0,ascending=False).head(10)\n",
      "    19                                                   \n",
      "    20         1        240.0    240.0      0.0      out = []\n",
      "    21        11     994519.0  90410.8      0.0      for index, row in top_emojis.iterrows():\n",
      "    22        10     523618.0  52361.8      0.0          out.append((row.loc[\"index\"], row.loc[0]))\n",
      "    23                                           \n",
      "    24         1        130.0    130.0      0.0      return out"
     ]
    }
   ],
   "source": [
    "from q2_memory import  q2_memory\n",
    "\n",
    "%lprun -f q2_memory q2_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/sandbox/files/latam-challenge/src/q2_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     7    308.1 MiB    308.1 MiB           1   def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
      "     8    308.1 MiB      0.0 MiB           1       result = dict()\n",
      "     9    308.1 MiB      0.0 MiB           2       with open(file_path, \"r\") as f:\n",
      "    10                                                 # En el archivo hay un json por linea, hay que itearr y extrar cada uno\n",
      "    11    308.1 MiB      0.0 MiB      117408           for line in f:\n",
      "    12    308.1 MiB      0.0 MiB      117407               data = pd.json_normalize(json.loads(line))\n",
      "    13    308.1 MiB      0.0 MiB      234814               for c in data.content.to_list():\n",
      "    14    308.1 MiB      0.0 MiB      160329                   for e in emoji.emoji_list(c):\n",
      "    15    308.1 MiB      0.0 MiB       42922                       result[e[\"emoji\"]] = result.get(e[\"emoji\"], 0) + 1\n",
      "    16                                         \n",
      "    17                                         \n",
      "    18    308.1 MiB      0.0 MiB           1       top_emojis = pd.json_normalize(result).T.reset_index().sort_values(0,ascending=False).head(10)\n",
      "    19                                                 \n",
      "    20    308.1 MiB      0.0 MiB           1       out = []\n",
      "    21    308.1 MiB      0.0 MiB          11       for index, row in top_emojis.iterrows():\n",
      "    22    308.1 MiB      0.0 MiB          10           out.append((row.loc[\"index\"], row.loc[0]))\n",
      "    23                                         \n",
      "    24    308.1 MiB      0.0 MiB           1       return out"
     ]
    }
   ],
   "source": [
    "%mprun -f q2_memory q2_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('🙏', 5049),\n",
       " ('😂', 3072),\n",
       " ('🚜', 2972),\n",
       " ('🌾', 2182),\n",
       " ('🇮🇳', 2086),\n",
       " ('🤣', 1668),\n",
       " ('✊', 1651),\n",
       " ('❤️', 1382),\n",
       " ('🙏🏻', 1317),\n",
       " ('💚', 1040)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2_time(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('🙏', 5049),\n",
       " ('😂', 3072),\n",
       " ('🚜', 2972),\n",
       " ('🌾', 2182),\n",
       " ('🇮🇳', 2086),\n",
       " ('🤣', 1668),\n",
       " ('✊', 1651),\n",
       " ('❤️', 1382),\n",
       " ('🙏🏻', 1317),\n",
       " ('💚', 1040)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2_memory(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pregunta 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problemática similar al ejercicio anterior, salvo por la diferencia de que el dato venía dentro de una lista de JSONs y no en un texto plano (exceptuando el hecho de que se tenían que extraer emojis).\n",
    "\n",
    "Por esta razón, se tomó un enfoque similar al de la pregunta anterior, con resultados similares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 2.09702 s\n",
      "File: /home/sandbox/files/latam-challenge/src/q3_time.py\n",
      "Function: q3_time at line 25\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    25                                           def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
      "    26         1  109873134.0    1e+08      5.2      spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
      "    27                                           \n",
      "    28         1  691153959.0    7e+08     33.0      data = spark.read.json(file_path)\n",
      "    29         1   40230284.0    4e+07      1.9      dataframe = get_most_mentions(spark, data)\n",
      "    30                                           \n",
      "    31         1        722.0    722.0      0.0      del data\n",
      "    32                                           \n",
      "    33         2      12412.0   6206.0      0.0      output = [\n",
      "    34         1 1011795270.0    1e+09     48.2          tuple(row) for row in dataframe.select([col(\"username\"), col(\"count\")]).collect()\n",
      "    35                                               ]\n",
      "    36                                           \n",
      "    37         1  243952115.0    2e+08     11.6      spark.stop()\n",
      "    38                                           \n",
      "    39         1        180.0    180.0      0.0      return output"
     ]
    }
   ],
   "source": [
    "from q3_time import  q3_time\n",
    "\n",
    "%lprun -f q3_time q3_time(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/sandbox/files/latam-challenge/src/q3_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    25    308.1 MiB    308.1 MiB           1   def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
      "    26    308.1 MiB      0.0 MiB           1       spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
      "    27                                         \n",
      "    28    308.1 MiB      0.0 MiB           1       data = spark.read.json(file_path)\n",
      "    29    308.1 MiB      0.0 MiB           1       dataframe = get_most_mentions(spark, data)\n",
      "    30                                         \n",
      "    31    308.1 MiB      0.0 MiB           1       del data\n",
      "    32                                         \n",
      "    33    308.1 MiB      0.0 MiB          24       output = [\n",
      "    34    308.1 MiB      0.0 MiB          11           tuple(row) for row in dataframe.select([col(\"username\"), col(\"count\")]).collect()\n",
      "    35                                             ]\n",
      "    36                                         \n",
      "    37    308.1 MiB      0.0 MiB           1       spark.stop()\n",
      "    38                                         \n",
      "    39    308.1 MiB      0.0 MiB           1       return output"
     ]
    }
   ],
   "source": [
    "%mprun -f q3_time q3_time(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 307.037 s\n",
      "File: /home/sandbox/files/latam-challenge/src/q3_memory.py\n",
      "Function: q3_memory at line 6\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     6                                           def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
      "     7         1       2164.0   2164.0      0.0      result = dict()\n",
      "     8         2     136466.0  68233.0      0.0      with open(file_path, \"r\") as f:\n",
      "     9                                                   # En el archivo hay un json por linea, hay que itearr y extrar cada uno\n",
      "    10    117408 1815098362.0  15459.8      0.6          for line in f:\n",
      "    11    117407        3e+11    3e+06     97.3              data = pd.json_normalize(json.loads(line))\n",
      "    12    117407 3005368140.0  25597.9      1.0              if \"quotedTweet.mentionedUsers\" in data.columns: # Hay registros que no tienen la columna\n",
      "    13     41436 3248181457.0  78390.3      1.1                  if data[\"quotedTweet.mentionedUsers\"][0]: # Y otros que vienen con \"None\"\n",
      "    14     23579  137980985.0   5851.9      0.0                      for u in data[\"quotedTweet.mentionedUsers\"][0]:\n",
      "    15     16075   20213130.0   1257.4      0.0                          result[u[\"username\"]] = result.get(u[\"username\"], 0) + 1\n",
      "    16                                               \n",
      "    17         1   76046699.0    8e+07      0.0      top_mentions = pd.json_normalize(result).T.reset_index().sort_values(0,ascending=False).head(10)\n",
      "    18                                                   \n",
      "    19         1        221.0    221.0      0.0      out = []\n",
      "    20        11     764049.0  69459.0      0.0      for index, row in top_mentions.iterrows():\n",
      "    21        10     442548.0  44254.8      0.0          out.append((row.loc[\"index\"], row.loc[0]))\n",
      "    22                                           \n",
      "    23         1        171.0    171.0      0.0      return out"
     ]
    }
   ],
   "source": [
    "from q3_memory import  q3_memory\n",
    "\n",
    "%lprun -f q3_memory q3_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/sandbox/files/latam-challenge/src/q3_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     6    308.2 MiB    308.2 MiB           1   def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
      "     7    308.2 MiB      0.0 MiB           1       result = dict()\n",
      "     8    314.0 MiB      0.0 MiB           2       with open(file_path, \"r\") as f:\n",
      "     9                                                 # En el archivo hay un json por linea, hay que itearr y extrar cada uno\n",
      "    10    314.0 MiB      0.0 MiB      117408           for line in f:\n",
      "    11    314.0 MiB      5.7 MiB      117407               data = pd.json_normalize(json.loads(line))\n",
      "    12    314.0 MiB      0.0 MiB      117407               if \"quotedTweet.mentionedUsers\" in data.columns: # Hay registros que no tienen la columna\n",
      "    13    314.0 MiB      0.0 MiB       41436                   if data[\"quotedTweet.mentionedUsers\"][0]: # Y otros que vienen con \"None\"\n",
      "    14    314.0 MiB      0.0 MiB       23579                       for u in data[\"quotedTweet.mentionedUsers\"][0]:\n",
      "    15    314.0 MiB      0.0 MiB       16075                           result[u[\"username\"]] = result.get(u[\"username\"], 0) + 1\n",
      "    16                                             \n",
      "    17    314.0 MiB      0.0 MiB           1       top_mentions = pd.json_normalize(result).T.reset_index().sort_values(0,ascending=False).head(10)\n",
      "    18                                                 \n",
      "    19    314.0 MiB      0.0 MiB           1       out = []\n",
      "    20    314.0 MiB      0.0 MiB          11       for index, row in top_mentions.iterrows():\n",
      "    21    314.0 MiB      0.0 MiB          10           out.append((row.loc[\"index\"], row.loc[0]))\n",
      "    22                                         \n",
      "    23    314.0 MiB      0.0 MiB           1       return out"
     ]
    }
   ],
   "source": [
    "%mprun -f q3_memory q3_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('narendramodi', 358),\n",
       " ('DelhiPolice', 320),\n",
       " ('nytimes', 312),\n",
       " ('GretaThunberg', 226),\n",
       " ('Kisanektamorcha', 205),\n",
       " ('jazzyb', 204),\n",
       " ('RakeshTikaitBKU', 204),\n",
       " ('mujerxsrising', 173),\n",
       " ('Bkuektaugrahan', 166),\n",
       " ('sushant_says', 157)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3_time(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('narendramodi', 358),\n",
       " ('DelhiPolice', 320),\n",
       " ('nytimes', 312),\n",
       " ('GretaThunberg', 226),\n",
       " ('Kisanektamorcha', 205),\n",
       " ('RakeshTikaitBKU', 204),\n",
       " ('jazzyb', 204),\n",
       " ('mujerxsrising', 173),\n",
       " ('Bkuektaugrahan', 166),\n",
       " ('sushant_says', 157)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q3_memory(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
