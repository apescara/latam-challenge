{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu solución y todas las suposiciones que estás considerando. Aquí puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pregunta 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se tomaron dos enfoques distintos. El primero consistió en leer los datos con la librería JSON y luego trabajar la información con Pandas. Para el segundo enfoque se utilizó PySpark para la lectura y tratamiento de los datos.\n",
    "\n",
    "En cuanto al tiempo de ejecución con el primer método, se observó que los tiempos eran estables y repetibles, con una duración de alrededor de 11 segundos. En cambio, el segundo método dependía de forma sustancial del estado de la sesión de Spark. En la primera ejecución consecutiva, los tiempos de ejecución rondaban los 15 segundos, muy distintos a los tiempos de las ejecuciones posteriores, que rondaban entre 5 y 6 segundos.\n",
    "\n",
    "Debido a esto, se tomó la decisión de elegir el primer método para q1_time, únicamente por su estabilidad. Cabe considerar que, en el caso de tener un ambiente de ejecución más estable, el segundo método podría ofrecer mejores resultados a largo plazo.\n",
    "\n",
    "En cuanto al uso de memoria, este fue un análisis más directo. Con el método 1, siempre se observó un uso de hasta 1300 MiB, muy por encima del método 2, que rondaba los 300 MiB de forma constante. Por ende, se seleccionó el método 2 para q1_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metodo 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ejecuto multiples veces y siempre se consigui un tiempo de ejecucion de alrededor de 11s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q1_time import  q1_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 11.0353 s\n",
      "File: /home/sandbox/files/latam-challenge/src/q1_time.py\n",
      "Function: q1_time at line 8\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     8                                           def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "     9                                           \n",
      "    10         1       1182.0   1182.0      0.0      def read_json(file_path):\n",
      "    11                                                   data = []\n",
      "    12                                                   with open(file_path, \"r\") as f:\n",
      "    13                                                       # En el archivo hay un json por linea, hay que itearr y extrar cada uno\n",
      "    14                                                       for line in f:\n",
      "    15                                                           data.append(json.loads(line))\n",
      "    16                                                       f.close()\n",
      "    17                                           \n",
      "    18                                                   return data\n",
      "    19                                           \n",
      "    20                                               # Leer json y pasar a DF\n",
      "    21         1 6526351664.0    7e+09     59.1      data = read_json(file_path=file_path)\n",
      "    22         1  772109664.0    8e+08      7.0      data = pd.DataFrame.from_records(data)\n",
      "    23                                           \n",
      "    24                                               # Arreglar datos a utilizar\n",
      "    25         1 3453171372.0    3e+09     31.3      data[\"date_part\"] = data.date.apply(lambda x: pendulum.parse(x).date())\n",
      "    26         1   64078844.0    6e+07      0.6      data[\"username\"] = data.user.apply(lambda x: x[\"username\"])\n",
      "    27                                           \n",
      "    28                                               # Seleccionar y agrupar data a usar\n",
      "    29         1        351.0    351.0      0.0      data_group = (\n",
      "    30         1    6260832.0    6e+06      0.1          data[[\"date_part\", \"username\", \"id\"]]\n",
      "    31         1     411679.0 411679.0      0.0          .groupby([\"date_part\", \"username\"])\n",
      "    32         1   80349934.0    8e+07      0.7          .count()\n",
      "    33         1    6810530.0    7e+06      0.1          .reset_index()\n",
      "    34         1    5809088.0    6e+06      0.1          .sort_values(by=\"id\", ascending=False)\n",
      "    35                                               )\n",
      "    36                                           \n",
      "    37                                               # Calcular resultados, se obtendra por un lado los dias mas activos y por el otro los usuarios mas activos por cada día.\n",
      "    38                                               # Finalmente se realizara el cruce de los 2\n",
      "    39         1        140.0    140.0      0.0      most_active_dates = (\n",
      "    40         1     371425.0 371425.0      0.0          data_group.groupby(\"date_part\")[\"id\"]\n",
      "    41         1    3165197.0    3e+06      0.0          .sum()\n",
      "    42         1     198000.0 198000.0      0.0          .sort_values(ascending=False)\n",
      "    43         1      80521.0  80521.0      0.0          .head(10)\n",
      "    44         1       1393.0   1393.0      0.0          .index\n",
      "    45                                               )\n",
      "    46         1        171.0    171.0      0.0      most_active_user_by_date = (\n",
      "    47         1     191358.0 191358.0      0.0          data_group.groupby([\"date_part\", \"username\"])[\"id\"]\n",
      "    48         1   91447367.0    9e+07      0.8          .sum()\n",
      "    49         1     799014.0 799014.0      0.0          .sort_values(ascending=False)\n",
      "    50         1     425886.0 425886.0      0.0          .groupby(level=0)\n",
      "    51         1   20543119.0    2e+07      0.2          .idxmax()\n",
      "    52         1     386322.0 386322.0      0.0          .apply(lambda x: x[1])\n",
      "    53                                               )\n",
      "    54         2     513800.0 256900.0      0.0      most_active_dates_with_most_active_users = most_active_user_by_date.loc[\n",
      "    55         1        120.0    120.0      0.0          most_active_dates\n",
      "    56                                               ]\n",
      "    57                                           \n",
      "    58                                               # Dar formato final\n",
      "    59         1        230.0    230.0      0.0      out = []\n",
      "    60        11    1475037.0 134094.3      0.0      for index, row in most_active_dates_with_most_active_users.reset_index().iterrows():\n",
      "    61        10     343882.0  34388.2      0.0          out.append((row.iloc[0], row.iloc[1]))\n",
      "    62                                           \n",
      "    63         1        170.0    170.0      0.0      return out"
     ]
    }
   ],
   "source": [
    "%lprun -f q1_time q1_time(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 10.5336 s\n",
      "File: /home/sandbox/files/latam-challenge/src/q1_time.py\n",
      "Function: q1_time at line 8\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     8                                           def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "     9                                           \n",
      "    10         1       1182.0   1182.0      0.0      def read_json(file_path):\n",
      "    11                                                   data = []\n",
      "    12                                                   with open(file_path, \"r\") as f:\n",
      "    13                                                       # En el archivo hay un json por linea, hay que itearr y extrar cada uno\n",
      "    14                                                       for line in f:\n",
      "    15                                                           data.append(json.loads(line))\n",
      "    16                                                       f.close()\n",
      "    17                                           \n",
      "    18                                                   return data\n",
      "    19                                           \n",
      "    20                                               # Leer json y pasar a DF\n",
      "    21         1 6480833095.0    6e+09     61.5      data = read_json(file_path=file_path)\n",
      "    22         1  792292537.0    8e+08      7.5      data = pd.DataFrame.from_records(data)\n",
      "    23                                           \n",
      "    24                                               # Arreglar datos a utilizar\n",
      "    25         1 3015026849.0    3e+09     28.6      data[\"date_part\"] = data.date.apply(lambda x: pendulum.parse(x).date())\n",
      "    26         1   73338453.0    7e+07      0.7      data[\"username\"] = data.user.apply(lambda x: x[\"username\"])\n",
      "    27                                           \n",
      "    28                                               # Seleccionar y agrupar data a usar\n",
      "    29         1        410.0    410.0      0.0      data_group = (\n",
      "    30         1    7546235.0    8e+06      0.1          data[[\"date_part\", \"username\", \"id\"]]\n",
      "    31         1     378407.0 378407.0      0.0          .groupby([\"date_part\", \"username\"])\n",
      "    32         1   80445916.0    8e+07      0.8          .count()\n",
      "    33         1    6764604.0    7e+06      0.1          .reset_index()\n",
      "    34         1    3673807.0    4e+06      0.0          .sort_values(by=\"id\", ascending=False)\n",
      "    35                                               )\n",
      "    36                                           \n",
      "    37                                               # Calcular resultados, se obtendra por un lado los dias mas activos y por el otro los usuarios mas activos por cada día.\n",
      "    38                                               # Finalmente se realizara el cruce de los 2\n",
      "    39         1        141.0    141.0      0.0      most_active_dates = (\n",
      "    40         1     258353.0 258353.0      0.0          data_group.groupby(\"date_part\")[\"id\"]\n",
      "    41         1    2943021.0    3e+06      0.0          .sum()\n",
      "    42         1     191758.0 191758.0      0.0          .sort_values(ascending=False)\n",
      "    43         1      60673.0  60673.0      0.0          .head(10)\n",
      "    44         1       1442.0   1442.0      0.0          .index\n",
      "    45                                               )\n",
      "    46         1        240.0    240.0      0.0      most_active_user_by_date = (\n",
      "    47         1     237143.0 237143.0      0.0          data_group.groupby([\"date_part\", \"username\"])[\"id\"]\n",
      "    48         1   43465724.0    4e+07      0.4          .sum()\n",
      "    49         1     759870.0 759870.0      0.0          .sort_values(ascending=False)\n",
      "    50         1     383507.0 383507.0      0.0          .groupby(level=0)\n",
      "    51         1   22271580.0    2e+07      0.2          .idxmax()\n",
      "    52         1     324046.0 324046.0      0.0          .apply(lambda x: x[1])\n",
      "    53                                               )\n",
      "    54         2     452735.0 226367.5      0.0      most_active_dates_with_most_active_users = most_active_user_by_date.loc[\n",
      "    55         1        170.0    170.0      0.0          most_active_dates\n",
      "    56                                               ]\n",
      "    57                                           \n",
      "    58                                               # Dar formato final\n",
      "    59         1        230.0    230.0      0.0      out = []\n",
      "    60        11    1554785.0 141344.1      0.0      for index, row in most_active_dates_with_most_active_users.reset_index().iterrows():\n",
      "    61        10     431774.0  43177.4      0.0          out.append((row.iloc[0], row.iloc[1]))\n",
      "    62                                           \n",
      "    63         1        161.0    161.0      0.0      return out"
     ]
    }
   ],
   "source": [
    "%lprun -f q1_time q1_time(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/sandbox/files/latam-challenge/src/q1_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     8    268.5 MiB    268.5 MiB           1   def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "     9                                         \n",
      "    10    268.5 MiB      0.0 MiB           2       def read_json(file_path):\n",
      "    11    268.5 MiB      0.0 MiB           1           data = []\n",
      "    12   1188.1 MiB      0.0 MiB           2           with open(file_path, \"r\") as f:\n",
      "    13                                                     # En el archivo hay un json por linea, hay que itearr y extrar cada uno\n",
      "    14   1188.1 MiB     21.4 MiB      117408               for line in f:\n",
      "    15   1188.1 MiB    898.2 MiB      117407                   data.append(json.loads(line))\n",
      "    16   1188.1 MiB      0.0 MiB           1               f.close()\n",
      "    17                                         \n",
      "    18   1188.1 MiB      0.0 MiB           1           return data\n",
      "    19                                         \n",
      "    20                                             # Leer json y pasar a DF\n",
      "    21   1188.1 MiB      0.0 MiB           1       data = read_json(file_path=file_path)\n",
      "    22   1270.5 MiB     82.5 MiB           1       data = pd.DataFrame.from_records(data)\n",
      "    23                                         \n",
      "    24                                             # Arreglar datos a utilizar\n",
      "    25   1279.5 MiB      9.0 MiB      234815       data[\"date_part\"] = data.date.apply(lambda x: pendulum.parse(x).date())\n",
      "    26   1279.5 MiB      0.0 MiB      234815       data[\"username\"] = data.user.apply(lambda x: x[\"username\"])\n",
      "    27                                         \n",
      "    28                                             # Seleccionar y agrupar data a usar\n",
      "    29   1279.5 MiB      0.0 MiB           1       data_group = (\n",
      "    30   1279.5 MiB      0.0 MiB           1           data[[\"date_part\", \"username\", \"id\"]]\n",
      "    31   1279.5 MiB      0.0 MiB           1           .groupby([\"date_part\", \"username\"])\n",
      "    32   1279.5 MiB      0.0 MiB           1           .count()\n",
      "    33   1279.5 MiB      0.0 MiB           1           .reset_index()\n",
      "    34   1279.5 MiB      0.0 MiB           1           .sort_values(by=\"id\", ascending=False)\n",
      "    35                                             )\n",
      "    36                                         \n",
      "    37                                             # Calcular resultados, se obtendra por un lado los dias mas activos y por el otro los usuarios mas activos por cada día.\n",
      "    38                                             # Finalmente se realizara el cruce de los 2\n",
      "    39   1279.5 MiB      0.0 MiB           1       most_active_dates = (\n",
      "    40   1279.5 MiB      0.0 MiB           1           data_group.groupby(\"date_part\")[\"id\"]\n",
      "    41   1279.5 MiB      0.0 MiB           1           .sum()\n",
      "    42   1279.5 MiB      0.0 MiB           1           .sort_values(ascending=False)\n",
      "    43   1279.5 MiB      0.0 MiB           1           .head(10)\n",
      "    44   1279.5 MiB      0.0 MiB           1           .index\n",
      "    45                                             )\n",
      "    46   1279.5 MiB      0.0 MiB           1       most_active_user_by_date = (\n",
      "    47   1279.5 MiB      0.0 MiB           1           data_group.groupby([\"date_part\", \"username\"])[\"id\"]\n",
      "    48   1279.5 MiB      0.0 MiB           1           .sum()\n",
      "    49   1279.5 MiB      0.0 MiB           1           .sort_values(ascending=False)\n",
      "    50   1279.5 MiB      0.0 MiB           1           .groupby(level=0)\n",
      "    51   1279.5 MiB      0.0 MiB           1           .idxmax()\n",
      "    52   1279.5 MiB      0.0 MiB          27           .apply(lambda x: x[1])\n",
      "    53                                             )\n",
      "    54   1279.5 MiB      0.0 MiB           2       most_active_dates_with_most_active_users = most_active_user_by_date.loc[\n",
      "    55   1279.5 MiB      0.0 MiB           1           most_active_dates\n",
      "    56                                             ]\n",
      "    57                                         \n",
      "    58                                             # Dar formato final\n",
      "    59   1279.5 MiB      0.0 MiB           1       out = []\n",
      "    60   1279.5 MiB      0.0 MiB          11       for index, row in most_active_dates_with_most_active_users.reset_index().iterrows():\n",
      "    61   1279.5 MiB      0.0 MiB          10           out.append((row.iloc[0], row.iloc[1]))\n",
      "    62                                         \n",
      "    63   1279.5 MiB      0.0 MiB           1       return out"
     ]
    }
   ],
   "source": [
    "%mprun -f q1_time q1_time(file_path=file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mem pregunta 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q1_memory import  q1_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/10/13 22:35:08 WARN Utils: Your hostname, pescara-pc resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/10/13 22:35:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/13 22:35:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 15.2764 s\n",
      "File: /home/sandbox/files/latam-challenge/src/q1_memory.py\n",
      "Function: q1_memory at line 44\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    44                                           def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    45                                           \n",
      "    46         1 2976992294.0    3e+09     19.5      spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
      "    47                                           \n",
      "    48         1 5282421951.0    5e+09     34.6      data = spark.read.json(file_path)\n",
      "    49         1  443077759.0    4e+08      2.9      dataframe = get_dates_and_users(spark, data)\n",
      "    50         1        481.0    481.0      0.0      del data\n",
      "    51                                           \n",
      "    52         2      15108.0   7554.0      0.0      output = [\n",
      "    53                                                   tuple(row)\n",
      "    54         1 5587778686.0    6e+09     36.6          for row in dataframe.select([col(\"date_part\"), col(\"username\")]).collect()\n",
      "    55                                               ]\n",
      "    56                                           \n",
      "    57         1  986108445.0    1e+09      6.5      spark.stop()\n",
      "    58                                           \n",
      "    59         1        341.0    341.0      0.0      return output"
     ]
    }
   ],
   "source": [
    "%lprun -f q1_memory q1_memory(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 4.8001 s\n",
      "File: /home/sandbox/files/latam-challenge/src/q1_memory.py\n",
      "Function: q1_memory at line 44\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    44                                           def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    45                                           \n",
      "    46         1   89298043.0    9e+07      1.9      spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
      "    47                                           \n",
      "    48         1  828416290.0    8e+08     17.3      data = spark.read.json(file_path)\n",
      "    49         1  105950756.0    1e+08      2.2      dataframe = get_dates_and_users(spark, data)\n",
      "    50         1       2024.0   2024.0      0.0      del data\n",
      "    51                                           \n",
      "    52         2      10339.0   5169.5      0.0      output = [\n",
      "    53                                                   tuple(row)\n",
      "    54         1 2777823350.0    3e+09     57.9          for row in dataframe.select([col(\"date_part\"), col(\"username\")]).collect()\n",
      "    55                                               ]\n",
      "    56                                           \n",
      "    57         1  998598240.0    1e+09     20.8      spark.stop()\n",
      "    58                                           \n",
      "    59         1        170.0    170.0      0.0      return output"
     ]
    }
   ],
   "source": [
    "%lprun -f q1_memory q1_memory(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/sandbox/files/latam-challenge/src/q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    44    273.6 MiB    273.6 MiB           1   def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    45                                         \n",
      "    46    273.6 MiB      0.0 MiB           1       spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
      "    47                                         \n",
      "    48    273.6 MiB      0.0 MiB           1       data = spark.read.json(file_path)\n",
      "    49    273.6 MiB      0.0 MiB           1       dataframe = get_dates_and_users(spark, data)\n",
      "    50    273.6 MiB      0.0 MiB           1       del data\n",
      "    51                                         \n",
      "    52    273.6 MiB      0.0 MiB          24       output = [\n",
      "    53    273.6 MiB      0.0 MiB          10           tuple(row)\n",
      "    54    273.6 MiB      0.0 MiB          11           for row in dataframe.select([col(\"date_part\"), col(\"username\")]).collect()\n",
      "    55                                             ]\n",
      "    56                                         \n",
      "    57    273.6 MiB      0.0 MiB           1       spark.stop()\n",
      "    58                                         \n",
      "    59    273.6 MiB      0.0 MiB           1       return output"
     ]
    }
   ],
   "source": [
    "%mprun -f q1_memory q1_memory(file_path=file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pregunta 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('🙏', 5049),\n",
       " ('😂', 3072),\n",
       " ('🚜', 2972),\n",
       " ('🌾', 2182),\n",
       " ('🇮🇳', 2086),\n",
       " ('🤣', 1668),\n",
       " ('✊', 1651),\n",
       " ('❤️', 1382),\n",
       " ('🙏🏻', 1317),\n",
       " ('💚', 1040)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from q2_time import  q2_time\n",
    "\n",
    "q2_time(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('🙏', 5049),\n",
       " ('😂', 3072),\n",
       " ('🚜', 2972),\n",
       " ('🌾', 2182),\n",
       " ('🇮🇳', 2086),\n",
       " ('🤣', 1668),\n",
       " ('✊', 1651),\n",
       " ('❤️', 1382),\n",
       " ('🙏🏻', 1317),\n",
       " ('💚', 1040)]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from q2_memory import  q2_memory\n",
    "\n",
    "q2_memory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
